paper_id,doc_id,title,doc
2112.06905,2112.06905_0,GLaM Efficient Scaling of Language Models with Mixture-of-Experts.pdf,"GLaM: Efficient Scaling of Language Models with Mixture-of-Experts
Nan Du“! Yanping Huang”! Andrew M. Dai”! Simon Tong! Dmitry Lepikhin' Yuanzhong Xu! Maxim Krikun! Yanqi Zhou! Adams Wei Yu! Orhan Firat! Barret Zoph! Liam Fedus! Maarten Bosma! Zongwei Zhou! Tao Wang! Yu Emma Wang! Kellie Webster! Marie Pellat'! Kevin Robinson! Kathleen Meier-Hellstern'! Toju Duke! Lucas Dixon! Kun Zhang! Quoc V Le! Yonghui Wu! Zhifeng Chen! Claire Cui!

Abstract
Scaling language models with more data, compute and parameters has driven significant progress in natural language processing. For example, thanks to scaling, GPT-3 was able to achieve strong re- sults on in-context learning tasks. However, train- ing these large dense models requires significant amounts of computing resources. In this paper, we propose and develop a family of language mod- els named GLaM (Generalist Language Model), which uses a sparsely activated mixture-of-experts architecture to scale the model capacity while also incurring substantially less training cost compared to dense variants. The largest GLaM has 1.2 tril- lion parameters, which is approximately 7x larger than GPT-3. It consumes only 1/3 of the energy used to train GPT-3 and requires half of the com- putation flops for inference, while still achieving better overall zero, one and few-shot performance across 29 NLP tasks.
Table 1. Comparison between GPT-3 and GLaM. In a nutshell, GLaM outperforms GPT-3 across 21 natural language understand- ing (NLU) benchmarks and 8 natural language generative (NLG) benchmarks in average while using about half the FLOPs per token during inference and consuming about one third the energy for training.
feasibility of in-context learning for few-shot or even zero- shot generalization, meaning very few labeled examples are needed to achieve good performance on NLP applications. While being effective and performant, scaling further is be- coming prohibitively expensive and consumes significant amounts of energy (Patterson et al., 2021)."
2112.06905,2112.06905_1,GLaM Efficient Scaling of Language Models with Mixture-of-Experts.pdf,"1. Introduction
Language models have played an important role in the progress of natural language processing (NLP) in the past decade. Variants of language models have been used to pro- duce pretrained word vectors (Mikolov et al., 2013; Penning- ton et al., 2014), and contextualized word vectors (Peters et al., 2018; Devlin et al., 2019) for many NLP applications. The shift towards scaling with more data and larger mod- els (Shazeer et al., 2017; Huang et al., 2019; Kaplan et al., 2020) has enabled complex natural language tasks to be per- formed with less labeled data. For example, GPT-3 (Brown et al., 2020) and FLAN (Wei et al., 2021) demonstrated the
“Equal contribution 'Google. Correspondence to: Nan Du, Yanping Huang, and Andrew M. Dai <dunan@google.com, huangyp @ google.com, adai@ google.com>.
Proceedings of the 39*"" International Conference on Machine Learning, Baltimore, Maryland, USA, PMLR 162, 2022. Copy- right 2022 by the author(s)."
2112.06905,2112.06905_2,GLaM Efficient Scaling of Language Models with Mixture-of-Experts.pdf,"Proceedings of the 39*"" International Conference on Machine Learning, Baltimore, Maryland, USA, PMLR 162, 2022. Copy- right 2022 by the author(s).
In this work, we show that a large sparsely activated network can achieve competitive results compared to state-of-the-art dense models on few-shot tasks while being more compu- tationally efficient. We present a family of generalist lan- guage models called GLaM, that strike a balance between dense and conditional computation. The largest version of GLaM has 1.2T parameters in total with 64 experts per MoE layer (Shazeer et al., 2017; Lepikhin et al., 2021; Fe- dus et al., 2021) where each token in the input batch only activates a subnetwork of 96.6B (8% of 1.2T) parameters. On zero, one and few-shot learning, this model compares favorably to GPT-3 (175B), with significantly improved learning efficiency across 29 public NLP benchmarks, rang- ing from language completion tasks, open-domain QA tasks, to natural language inference tasks. Thanks to the sparsely activated architecture and the efficient implementation of the model parallelism algorithm, the total energy consumption during training is only one third of GPT-3’s. We highlight the comparison between the largest version of GLaM and GPT-3 in Table 1 and Figure 1.
We use GLaM to study the importance of data. Our analysis shows that even for these large models, data quality should not be sacrificed for quantity if the goal is to produce a high- quality auto-regressive language model. More importantly, on social dimensions, our results are also the first, to our knowledge, to close the performance gap between stereo- typical and anti-stereotypical examples on the WinoGender benchmark, suggesting that large, sparsely activated models may rely less on superficial statistical correlations."
2112.06905,2112.06905_3,GLaM Efficient Scaling of Language Models with Mixture-of-Experts.pdf,"to improve various language understanding tasks. More re- cently, models that used Transformers (Vaswani et al., 2017) showed that larger models with self-supervision on unla- beled data could yield significant improvements on NLP tasks (Devlin et al., 2019; Yang et al., 2019; Liu et al., 2019; Clark et al., 2020). Transfer learning based on pre-training and finetuning (Raffel et al., 2020; Houlsby et al., 2019) has been extensively studied and demonstrated good perfor- mance on downstream tasks. However, a major limitation to this method is that it requires a task-specific fine-tuning.
Finally, although MoE-based sparse models are not yet com- mon in the NLP community, our work shows that sparse decoder-only language models can be more performant than the dense architectures of similar compute FLOPs for the first time within the few-shot in-context learning setting at scale, suggesting that sparsity is one of the most promising directions to achieve high-quality NLP models while saving energy costs (Patterson et al., 2021). MoE should therefore be considered as a strong candidate for future scaling.
In-Context Few-shot Learning. GPT-3 (Brown et al., 2020) and related work (Shoeybi et al., 2019; Lieber et al., 2021; Wei et al., 2021) demonstrated that scaling up lan- guage models greatly improves task-agnostic, few-shot per- formance. These language models are applied without any gradient updates, and only few-shot demonstrations speci- fied purely via text interactions with the model are needed."
2112.06905,2112.06905_4,GLaM Efficient Scaling of Language Models with Mixture-of-Experts.pdf,"2. Related Work
Language models. Neural language models (Mikolov et al., 2010; Sutskever et al., 2011) have been shown to be useful for many natural language processing tasks. Word em- bedding models and extensions such as word2vec (Mikolov et al., 2013), GloVe (Pennington et al., 2014) and paragraph vectors (Le & Mikolov, 2014) have shown good generaliza- tion to many tasks simply by transferring the embeddings.
Pre-training and Fine-tuning. The abundance of com- pute and data enables training increasingly large models via unsupervised pre-training. This is a natural fit for training neural networks as they exhibit remarkable scalability. Work on using recurrent models such as RNNs and LSTMs for language representation (Dai & Le, 2015; Kiros et al., 2015) showed that general language models could be fine-tuned
Sparsely Gated Networks. Mixture-of-Experts based models have also shown significant advantages. For lan- guage modeling and machine translation, Shazeer et al. (2017) showed that they could effectively use a very large number of weights while only needing to compute a small subset of the computation graph at inference time. There has also been work on scaling sparsely activated MoE ar- chitectures (Hestness et al., 2017; Shazeer et al., 2018; Lep- ikhin et al., 2021; Kudugunta et al., 2021). Recently, Fedus et al. (2021) showed results with even larger | trillion pa- rameter sparsely activated models (Switch-C). Although both Switch-C and the largest GLaM model have one tril- lion number of trainable parameters, GLaM is a family of decoder-only language models, and Switch-C is an encoder- decoder based sequence to sequence model. Furthermore, Switch-C is mainly evaluated on fine-tuning benchmarks, e.g., SuperGlue, while GLaM performs well without any"
2112.06905,2112.06905_5,GLaM Efficient Scaling of Language Models with Mixture-of-Experts.pdf,"Table 2. A sample of related models (Devlin et al., 2019; Raffel et al., 2020; Brown et al., 2020; Lieber et al., 2021; Rae et al., 2021; Shoeybi et al., 2019; Lepikhin et al., 2021; Fedus et al., 2021) pre-trained on text corpora. Nparams is the total number of trainable model parameters, Mact-params is the number of activated model parameters per input token.
need for fine-tuning in the few-shot setting shared by GPT-3 where SuperGlue is a subset. Table 2 summarizes the key differences between GLaM and related models pre-trained on text corpora."
2112.06905,2112.06905_6,GLaM Efficient Scaling of Language Models with Mixture-of-Experts.pdf,"3. Training Dataset
To train our model, we build a high-quality dataset of 1.6 trillion tokens that are representative of a wide range of natural language use cases. Web pages constitute the vast quantity of data in our unlabeled dataset. However, their quality ranges from professional writing to low-quality com- ment and forum pages. Similarly to Brown et al. (2020), we develop our own text quality classifier to produce a high- quality web corpus out of an original larger raw corpus. We use a feature hash based linear classifier for inference speed. This classifier is trained to classify between a collection of curated text (Wikipedia, books and a few selected web- sites) and other webpages. We use this classifier to estimate the content quality of a webpage. We then apply this clas- sifier by using a Pareto distribution to sample webpages according to their score. This allows some lower-quality webpages to be included to prevent systematic biases in the classifier (Brown et al., 2020).
are"
2112.06905,2112.06905_7,GLaM Efficient Scaling of Language Models with Mixture-of-Experts.pdf,"blue
Figure 2. GLaM model architecture. Each MoE layer (the bottom block) is interleaved with a Transformer layer (the upper block). For each input token, e.g., ‘roses’, the Gating module dynamically selects two most relevant experts out of 64, which is represented by the blue grid in the MoE layer. The weighted average of the outputs from these two experts will then be passed to the upper Transformer layer. For the next token in the input sequence, two different experts will be selected.
of webpages and combine this with books, Wikipedia pages, forums and news pages and other data sources to create the final GLaM dataset. We also incorporate the data from pub- lic domain social media conversations used by Adiwardana et al. (2020). We set the mixture weights based on the perfor- mance of each component in a smaller model and to prevent small sources such as Wikipedia from being over-sampled. Table 3 shows the details of our data component sizes and mixture weights. The mixture weights were chosen based on the performance of the component in a small model and to prevent small datasets such as Wikipedia from being over- sampled. To check data contamination, in Section D we conduct an overlap analysis between our training set and the evaluation data and find that it roughly matches that of previous work (Brown et al., 2020)."
2112.06905,2112.06905_8,GLaM Efficient Scaling of Language Models with Mixture-of-Experts.pdf,"4. Model Architecture
We leverage sparsely activated Mixture-of-Experts (MoE) (Shazeer et al., 2017; Fedus et al., 2021) in GLaM models. Similar to the GShard MoE Transformer (Lepikhin et al., 2021), we replace the feed-forward component of every other Transformer layer with an MoE layer, as shown in Figure 2. Each MoE layer consists of a collection of independent feed-forward networks as the ‘experts’. A
We use this process to generate a high-quality filtered subset
gating function then uses a softmax activation function to model a probability distribution over these experts. This distribution indicates how well each expert is able to process the incoming input.
Even though each MoE layer has many more parameters, the experts are sparsely activated. This means that for a given input token, only a limited subset of experts is used, giving the model more capacity while limiting computa- tion. In our architecture, the subset size is two!. Each MoE layer’s learnable gating network is trained to use its input to activate the best two experts for each token of an input sequence. During inference, the learned gating network dynamically picks the two best experts for each token. For an MoE layer with F experts, this essentially provides a collection of O(E) different combinations of feed-forward networks instead of one in the classic Transformer architec- ture, leading to much more computational flexibility. The final learned representation of a token will be the weighted combination of the outputs from the selected experts."
2112.06905,2112.06905_9,GLaM Efficient Scaling of Language Models with Mixture-of-Experts.pdf,"the hidden dimension of the feed-forward network, L is the number of layers and N is the number of total devices. Additionally, nparams is the total number of trainable model parameters, Mact-params is the number of activated model parameters per input token, Meads is the number of self- attention heads, and dyeaq is the hidden dimension of each attention head. We also include the respective dense models with comparable numbers of activated parameters per-token during inference (and thus similar numbers of per-token FLOPs) as references. We adopt the notation of
GLaM (Base Dense Size/E) e.g., GLaM (8B/64E)
to describe different variants in the GLaM models. For example, GLaM (8B/64E) represents the architecture of an approximate 8B parameter dense model with every other layer replaced by a 64 expert MoE layer. GLaM reduces to a dense Transformer-based language model architecture when each MoE layer only has one expert. We use the notation
We also make additional modifications to the original Trans- former architecture. We replace the standard positional embedding with per-layer relative positional bias from Dai et al. (2019). In the non-MoE Transformer feed-forward sub-layers, we replace the first linear projection and the ac- tivation function with the Gated Linear Unit (Dauphin et al., 2017; Shazeer, 2020), which computes the component-wise product of two linear transformation of the input, followed by a Gaussian Error Linear Unit (Hendrycks & Gimpel, 2016) activation function. We partition the weights and computation of large GLaM models using the 2D shard- ing algorithm as described in Xu et al. (2021), which is described in more details in the Section C of the appendix."
2112.06905,2112.06905_10,GLaM Efficient Scaling of Language Models with Mixture-of-Experts.pdf,"5. Experiment Setup
GLaM is a family of dense and sparse decoder-only lan- guage models, so we first elaborate our training settings, hyperparameters, and evaluation protocol in this section.

5.1. Training Setting
We train several variants of GLaM to study the behavior of MoE and dense models on the same training data. Table 4 shows the hyperparameter settings of different scale GLaM models ranging from 130 million parameters to 1.2 trillion parameters. Here, F is the number of experts in the MoE layer, B is the mini-batch size, S' is the input sequence length, M is the model and embedding dimension, H is
GLaM (Dense Size) e.g., GLaM (137B)
refers to a dense 137B parameter model trained with the same dataset."
2112.06905,2112.06905_11,GLaM Efficient Scaling of Language Models with Mixture-of-Experts.pdf,"5.2. Hyperparameters and Training Procedure
We use the same learning hyperparameters for all GLaM models. More specifically, We use a maximum sequence length of 1024 tokens, and pack each input example to have up to | million tokens per batch. The dropout rate is set to 0 since the number of available tokens in the training corpus is much greater than the number of processed tokens dur- ing training. Our optimizer is Adafactor (Shazeer & Stern, 2018) with first-moment decay 3; = 0, second-moment decay 82 = 0.99 with a 1 — t~°-® decay schedule, update clipping threshold of 1.0, and factored second-moment esti- mation. We keep the initial learning rate of 0.01 for the first 10K training steps, and then decay it with inverse square root schedule Ir(t) o 5. On top of the standard cross- entropy loss, we add the MoE auxiliary loss as described in GShard (Lepikhin et al., 2021) with a 0.01 coefficient to encourage expert load balancing so that the gating function will distribute tokens more evenly across all experts. We use the SentencePiece (Kudo & Richardson, 2018) subword to- kenizer with a vocabulary of size of 256K. During training, we use float32 for model weights and bfloat16 for activa- tions. The largest GLaM 64B/64E model was trained on 1,024 Cloud TPU-V4 chips.
‘Using more experts will cost more compute FLOPs per pre- diction, pushing the network to be ‘denser’. Setting the number of selected experts to be two is based on the trade-off between predictive performance and the training/serving efficiency of the model.
Training models at the trillion parameter scale is extremely expensive even for sparsely activated models. There is little room for hyperparameter tuning. Here we share our training recipes and some implementation tricks for the GLaM models.
¢ We train smaller-scale models to convergence first. This allows us to expose potential issues in the dataset and infrastructure as early as possible."
2112.06905,2112.06905_12,GLaM Efficient Scaling of Language Models with Mixture-of-Experts.pdf,"¢ We train smaller-scale models to convergence first. This allows us to expose potential issues in the dataset and infrastructure as early as possible.
guage understanding (NLU) tasks. These datasets can be further grouped into 7 categories and are listed in section A.
¢ We skip weight updates for a batch if there are any NaNs or Infs in the gradients (Shen et al., 2019). Note NaN/inf could still occur during the applying gradient step, in which case we restart from an earlier check- point as described below. For example, even if there is no Inf in the existing variable or the gradient, the updated variable could still lead to Inf.
Natural Language Generative tasks. We compare the language sequences decoded by the models to the ground truth in generative tasks. These tasks are TriviaQA, NQS, WebQS, SQuADv2, LAMBADA, DROP, QuAC and CoQA. The performance is measured by the accuracy of exact match (EM) and FI score, following the standard for each task in Brown et al. (2020). We use beam search with a width of 4 to generate the sequences.
¢ We restart from an early healthy checkpoint when en- countering rare large fluctuations or even NaN/Inf dur- ing training. Randomness of the sequentially loaded batches might help escape from previous failed states in the training after restart."
2112.06905,2112.06905_13,GLaM Efficient Scaling of Language Models with Mixture-of-Experts.pdf,"5.3. Evaluation Setting
Protocol. To clearly demonstrate the effectiveness of GLaM models, we mainly focus on evaluating the zero, one and few-shot learning protocols suggested by Radford et al. (2018); Brown et al. (2020). For the zero-shot learn- ing setting, in most cases, we evaluate each example in the development set directly. For one/few-shot learning, we mainly draw random one/few examples from that task’s training set as the only demonstration and context. Such a demonstration is concatenated with the evaluation example with two newlines in between, and then fed into the model.
Natural Language Understanding tasks. Most lan- guage understanding tasks require the model to select one correct answer from multiple options. All binary classifica- tion tasks are formulated into the form of selecting among two options (“Yes’ or ‘No’). The prediction is based on the maximum log-likelihood of each option given the context log P(option|context) normalized by the token length of each option. On a few tasks, such as ReCoRD (Zhang et al., 2018) and COPA (Gordon et al., 2012), the non-normalized loss can yield better results and thus is adopted. Except for MultiRC (Khashabi et al., 2018) where the F1 metric over the set of answer options (referred to as F1,,) is reported, the prediction accuracy metric is used for all the other tasks. We use the average of the scores reported in all datasets to report the overall few-shot performance of models on both NLG and NLU tasks. Both Accuracy (EM) and F1 scores have been normalized to lie between 0 and 100. On Trivi- aQA, we also report the testing server score of our one-shot submission."
2112.06905,2112.06905_14,GLaM Efficient Scaling of Language Models with Mixture-of-Experts.pdf,"Benchmarks. To allow for an apples-to-apples compari- son between GPT-3 and GLaM, we choose the same suite of evaluation tasks as Brown et al. (2020). But for sim- plicity, we exclude 7 synthetic tasks (arithmetic and word unscramble) and 6 machine translation datasets. With this exclusion, we end up with 29 datasets, which includes 8 natural language generative (NLG) tasks and 21 natural lan-"
2112.06905,2112.06905_15,GLaM Efficient Scaling of Language Models with Mixture-of-Experts.pdf,"6. Results
We conduct extensive evaluation on the whole family of GLaM models, to show the advantages of sparsely activated models in language modeling and their scaling trends. We
also quantitatively inspect the effectiveness of data quality for language model training.
Table 5. GLaM (64B/64E) one-shot performance significantly out- performs prior SOTAs for open domain settings in the wiki split."
2112.06905,2112.06905_16,GLaM Efficient Scaling of Language Models with Mixture-of-Experts.pdf,"6.1. Comparison between MoE and Dense Models
As previously presented in Table 1, GLaM (64B/64E) has competitive performance compared to GPT-3 (175B) for zero, one and few-shot learning. Figure 1 compares the performance for each category of tasks. In total, GLaM (64B/64E) outperforms GPT-3 in 6 out of 7 categories on average, indicating the performance gain is consistent. For more details on each individual task, see Table 11. We include results on the much larger and computationally de- manding Megatron-NLG and Gopher for reference. More importantly, as shown in Table 4, GLaM (64B/64E) acti- vates roughly 96.6B parameters per token during inference, which requires only half of the compute FLOPs needed by GPT-3 given the same input."
2112.06905,2112.06905_17,GLaM Efficient Scaling of Language Models with Mixture-of-Experts.pdf,"We highlight one particular challenging open-domain ques- tion answer task: TriviaQA. In open-domain question an- swer tasks, the model is required to directly answer a given query without access to any additional context. Brown et al. (2020) show that the few-shot performance of Trivi- aQA is able to grow smoothly with model size, indicating a language model is able to absorb knowledge using its model capacity. As shown in Table 5, GLaM (64B/64E) is better than the dense model and outperforms the previous finetuned state-of-the-art (SOTA) on this dataset in the open- domain setting. Our one-shot result exceeds the previous finetuned SOTA (Yu et al., 2022) where additional knowl- edge graph information is infused by 8.6%, and outperforms the few-shot GPT-3 on the testing server by 5.3%. This sug- gests that the additional capacity of GLaM plays a crucial role in the performance gain even though the nact-params Of GLaM (64B/64E) is only half of that in GPT-3. Comparing to Switch-C, even though both models have similar total number of parameters, GLaM (64B/64E) uses much larger experts (beyond one TPU core) than Switch-C. Therefore, GLaM’s one-shot performance on TriviaQA is also better than the fine-tuned results of Switch-C in the open-domain setting. Finally, we report zero, one and few-shot evaluation mainly on the development set for all tasks in Tables 11, 12, 13 and 14 of the appendix.
The filtered webpages consist of 143B tokens whereas the unfiltered webpages consist of around 7T tokens.
Figure 3 (c) and (d) show that the model trained on fil- tered data performs consistently better on both NLG and NLU tasks. In particular, the effect of filtering is bigger on NLG than that on NLU. Perhaps this is because NLG often requires generating high-quality language and filtered pretraining corpora is crucial to the generation capability of language models. Our study highlights the fact that the quality of the pretrained data also plays a critical role, specif- ically, in the performance of downstream tasks."
2112.06905,2112.06905_18,GLaM Efficient Scaling of Language Models with Mixture-of-Experts.pdf,"6.3. Scaling Studies
Scaling up dense language models generally involves mak- ing the models deeper by adding more layers, and wider by increasing the embedding dimension of token representa- tions. This process increases the total number of parameters Nparams Of the model. For each prediction on a given input example, these models are ‘dense’ in that all Mparams param- eters will be activated, i.e., Mparams = Nact-params In Table 4. Therefore, the effective FLOPs per prediction increases linearly with the model size Nparams. While the increased FLOPs may lead to boosted predictive performance, it also raises the overall cost per prediction.
In contrast, GLaM MoE models are sparsely activated in that only a small fraction of the total Mparams parameters will be activated for each prediction where Mparams >> Nact-params- Therefore, GLaM MoE models can scale by also growing the size or number of experts in the MoE layer."
2112.06905,2112.06905_19,GLaM Efficient Scaling of Language Models with Mixture-of-Experts.pdf,"6.2. Effect of Data Quality
We study the impact of data quality on the few-shot perfor- mance of downstream tasks. We use a modest-size GLaM model (1.7B/64E) to show the effectiveness of filtering text on model quality. We train models with the same hyper- parameters on two datasets. One is the original dataset described in Section 3 and the second consists of the dataset with the filtered webpages replaced with the unfiltered web- pages. The mixing proportions are fixed as given in Table 3.
As shown in Figure 3(a), the average zero, one and few-shot performance across the generative tasks scales well with the effective FLOPs per prediction which is in turn determined by Nact-params- We also find that GLaM MoE models perform consistently better than GLaM dense models for similar ef- ective FLOPs per token. For language understanding tasks shown in Figure 3(b), the performance gain of GLaM MoE models has a similar scaling trend to that of the generative tasks. We observe that both MoE and dense models perform similarly at smaller scales but MoE models outperform at
larger scales. We also show experiments with scaling the number of experts in Section B where we observe that, for a fixed budget of computation per prediction, adding more experts generally leads to better predictive performance."
2112.06905,2112.06905_20,GLaM Efficient Scaling of Language Models with Mixture-of-Experts.pdf,"6.4. Efficiency of GLaM
Existing large dense language models usually require tremendous amounts of computation resources for train- ing and serving (Patterson et al., 2021). They also need to consume massive amounts of pretraining data. We investi- gate the data and compute efficiency of the proposed GLaM models.
sparsely activated models takes much less computational resources than training dense models.
As previously presented in Table 1, the GLaM (64B/64E) training after 600B tokens consumes 456 MWh, about 1/3 of the energy cost of 1287 MWh used by GPT-3. Moreover, to reach similar (and slightly exceeded) scores as GPT-3, we train using 1,024 TPU-v4 chips for 574 hours (with 280B tokens). This consumes 213 MWh or 1/6 of the GPT-3 energy cost. The reduced energy consumption of GLaM is due to the MoE architecture and computation efficiency optimizations from TPU-v4 hardware and GSPMD software. Energy calculations can be found in Section F.
Data Efficiency. Figure 4 (a-c) and Figure 4(e-g) show the learning curves of our models compared to the dense baselines of similar effective FLOPs in both NLG and NLU tasks. The x-axis is the number of tokens used in train- ing where we explicitly include GPT-3’s results when it is around 300B tokens. We first observe that GLaM MoE models require significantly less data than dense models of comparable FLOPs to achieve similar zero, one, and few- shot performance. In other words, when the same amount of data is used for training, MoE models perform much bet- ter, and the difference in performance becomes larger when training up to 630B. Moreover, GLaM (64B/64E) model trained with 280B tokens outperforms GPT-3 trained with 300B tokens by large margins on 4 out of the 6 learning set- tings (zero-shot/one-shot NLU and one-shot/few-shot NLG), and matches GPT-3 scores for the remaining setting, i.e., zero-shot NLG tasks."
2112.06905,2112.06905_21,GLaM Efficient Scaling of Language Models with Mixture-of-Experts.pdf,"Computation Efficiency & Energy Consumption. Fig- ure 4 (d) and Figure 4 (h) show how the average zero, one and few-shot performance scales with the number of TPU years spent training MoE and dense models. We find that to achieve similar performance on downstream tasks, training"
2112.06905,2112.06905_22,GLaM Efficient Scaling of Language Models with Mixture-of-Experts.pdf,"7. Ethics and Unintended Biases
Large language models’ zero-and few-shot inference is an exciting capability: being able to control model behaviour intuitively with natural language and small datasets signifi- cantly lowers the barrier to prototyping and the development of new applications; it has the potential to help democratise using AI by dramatically decreasing the need for special- ist knowledge. However, such opportunities also serve to highlight the importance of the many ethical challenges (Leidner & Plachouras, 2017; Bender et al., 2021; Bom- masani et al., 2021) including representation bias (Blodgett et al., 2020), proper selection and handling of training data (Rogers, 2021) and its documentation (Bender & Friedman, 2018), privacy (Abadi et al., 2016b; Carlini et al., 2020), and environmental concerns (Strubell et al., 2019; Patterson et al., 2021). An important strand of this research focuses on unintended biases learnt by language models, includ- ing correlations between gender and profession (Bolukbasi et al., 2016; Rudinger et al., 2018; Zhao et al., 2018), neg- ative sentiment about racial and religious groups (Li et al., 2020; Nadeem et al., 2021), and about people with disabili- ties (Hutchinson et al., 2020), as well as other social biases
(Caliskan et al., 2017; Rudinger et al., 2017; Sap et al., 2020; Sotnikova et al., 2021). While measuring and mitigating the potential harm of language models is a very active area of research, as recognized by Blodgett et al. (2021); Jacobs & Wallach (2021) there is still a significant need for more rigorous evaluation methods to assess the degree to which language models encode harmful stereotypes (May et al., 2019; Webster et al., 2021)."
2112.06905,2112.06905_23,GLaM Efficient Scaling of Language Models with Mixture-of-Experts.pdf,"While there is not yet consensus on measurement methods or criteria for such general purpose large language models, the versatility and power of these models make it important to assess them on a range of metrics. We take inspiration from GPT-3 (Brown et al., 2020) and examine the co-occurrence in generated text referencing identity terms as well as report on the WinoGender benchmark (Rudinger et al., 2018). We also analyse toxicity degeneration similarly to Gopher (Rae et al., 2021), and extend the analysis to consider the human- behavioral baseline.
the appendix), 800 outputs are generated using top-k sam- pling (k = 40) with a temperature of 1. An off-the-shelf POS tagger (Bird & Loper, 2004) is used to remove stop words and select only descriptive words (i.e., adjectives and adverbs). Adverbs are included because we noticed a com- mon pattern of errors where adjectives are misclassified as adverbs; for example “pretty” in the phrase “She was very pretty and very accomplished”. Like Brown et al. (2020), to make the analysis transparent and easily reproducible, we omit any manual human labeling.
Like the analysis of other large language models that we build on, we note associative biases for all dimensions are obvious, for example “pretty” is the most associated descrip- tion for the term “She”, while it is not in the top-10 for the term “He”. Table 8 shows the most frequently occurring descriptive words in response to prompt-templates for gen- dered pronouns, and Tables 9 and 10 of the appendix show the same for race and religion prompts."
2112.06905,2112.06905_24,GLaM Efficient Scaling of Language Models with Mixture-of-Experts.pdf,"7.1. Co-occurrence prompts

7.2. WinoGender
Following the procedure described in Brown et al. (2020), we analyze commonly co-occurring words in the continua- tions when given prompts like “{term} was very...” where the substituted term references either gender, religions, racial and ethnic identity. For each prompt (Table 7 of
Coreference resolution is a capability that many applica- tions require to perform well, including machine translation (Stanovsky et al., 2019; Webster & Pitler, 2020) and ques- tion answering (Lamm et al., 2020). To assess whether gendered correlations in GLaM cause it to make corefer-
ence errors in the one-shot setting, we measure WinoGender (Rudinger et al., 2018). GLaM (64B/64E) achieves a new state-of-the-art of 71.7% on the full dataset (compared to 64.2% for GPT-3 (Brown et al., 2020)). Promisingly, ac- curacy is remarkably close between ‘he’ examples (70.8%) and ‘she’ examples (72.5%), as well as between stereotyp- ical examples (where the intended distribution is assumed to be close to the US occupation statistics, (Rudinger et al., 2018)) and anti-stereotypical (or ‘gotcha’) examples (both 71.7%)."
2112.06905,2112.06905_25,GLaM Efficient Scaling of Language Models with Mixture-of-Experts.pdf,"7.3. Toxicity Degeneration
Toxicity degeneration is when a language model produces text that is unintentionally toxic. To evaluate toxicity de- generation, we adapt the methodology used in (Welb] et al., 2021; Rae et al., 2021). We use the RealToxicityPrompts dataset (Gehman et al., 2020) which consists of sentences that have been split into two parts: a prompt prefix, and a continuation postfix. Like the previous studies, we also use the Perspective API which assigns a probability that the text would be considered to be rude, disrespectful or otherwise likely to make people want to leave a conversation. We then asses how likely a continuation is to be toxic given various likelihoods that the prompt was toxic.
For each of 10K randomly sampled prompts, we generate 25 continuations, with up to 100 tokens per continuations using top-k sampling (k = 40) with a temperature of 1. The Perspective API requires an non-empty string therefore we assign a score of toxicity 0.0 when the continuation is the empty string; this could represent, for example, a chat bot simply refusing to respond.
Figure 5 shows the relationship between the Toxicity Proba- bility of the Prompt (TPP), and the Toxicity Probability of the Continuation (TPC). Note that, for low TPP, the rela- tively high human TPC is due to the sampling strategy used
to create the underlying dataset: sentences were selected across the toxicity spectrum. Moreover, toxicity can often be identified locally within a sentence, and toxicity in this dataset tends to occur later the sentences. This causes the human-TPC to slightly drop as the TPP increases. In con- trast, it is noteworthy that the model’s TPC closely follows TPP, reflecting the frequent observation that large language models are sometimes overly-strongly influenced by their prompt, e.g. repeating phrases from the prompt."
2112.06905,2112.06905_26,GLaM Efficient Scaling of Language Models with Mixture-of-Experts.pdf,"We also analysed the distribution of toxicity probabilities from the API for batches of 25 continuations. This high- lighted that, even for low toxicity prompts, it is very likely that some generated continuation will be judged as toxic by most people reviewing it, according to the Perspective API’s predicted probability; further details can be found in Figure 8. We also note that this dataset’s sampling strategy, and the source it is taken from (Reddit) are likely not reflective of other domains. Moreover, even for very low TPP, applica- tions are likely to want a much lower TPC: even generating 1 in 100 toxic suggestions is likely to be very problematic for applications."
2112.06905,2112.06905_27,GLaM Efficient Scaling of Language Models with Mixture-of-Experts.pdf,"8. Discussion
As observed in previous work on sparsely-activated mod- els (Fedus et al., 2021), MoE models are more performant in knowledge-oriented tasks. Open-domain tasks are one way of measuring the amount of knowledge stored in a model. The performance of the MoE model in open-domain QA benchmarks such as TriviaQA demonstrate the significantly increased information capacity of these models compared to dense models of similar effective FLOPs. Despite the in-context learning and training efficiency advantages, the sparsely activated models consist of a higher number of pa- rameters and thus require a larger number of devices. This limits the resource accessibility and increases the serving cost especially when the serving traffic is low.

9. Conclusions
We propose and develop a family of generalist language models called GLaM, which use a sparsely activated mixture-of-experts architecture to achieve better average scores than not only their dense counterparts of similar effec- tive FLOPs, but also the GPT-3 models on 29 representative NLP tasks in zero, one and few-shot learning. In partic- ular, GLaM (64B/64E), our largest 1.2 trillion parameter MoE language model, achieves better average performance with only one third of energy consumption compared to training GPT-3. We hope that our work will encourage more research into methods for obtaining high-quality data, and using MoE for more efficient scaling of giant language models."
2112.06905,2112.06905_28,GLaM Efficient Scaling of Language Models with Mixture-of-Experts.pdf,"Abadi, M., Barham, P., Chen, J., Chen, Z., Davis, A., Dean, J., Devin, M., Ghemawat, S., Irving, G., Isard, M., Kudlur, M., Levenberg, J., Monga, R., Moore, S., Murray, D. G., Steiner, B., Tucker, P., Vasudevan, V., Warden, P., Wicke, M., Yu, Y., and Zheng, X. TensorFlow: A system for Large-Scale machine learning. In /2th USENIX Symposium on Operating Systems Design and Implementation (OSDI 16), pp. 265-283, Savannah, GA, November 2016a. USENIX Association. ISBN 978-1- 931971-33-1. URL https://www.usenix.org/ conference/osdil6/technical-sessions/ presentation/abadi.
Abadi, M., Chu, A., Goodfellow, I., McMahan, H. B., Mironov, I., Talwar, K., and Zhang, L. Deep learning with differential privacy. Proceedings of the 2016 ACM SIGSAC Conference on Computer and Communications Security, Oct 2016b. doi: 10.1145/2976749.2978318. URL http://dx.doi.org/10.1145/2976749. 2978318.
Adiwardana, D., Luong, M., So, D. R., Hall, J., Fiedel, N., Thoppilan, R., Yang, Z., Kulshreshtha, A., Nemade, G., Lu, Y., and Le, Q. V. Towards a human-like open- domain chatbot. CoRR, abs/2001.09977, 2020. URL https://arxiv.org/abs/2001.09977.
2004. Association for Computational Linguistics. URL https://aclanthology.org/P04-3031.
Bisk, Y., Zellers, R., Bras, R. L., Gao, J., and Choi, Y. Piqa: Reasoning about physical commonsense in natural language. In Thirty-Fourth AAAI Conference on Artificial Intelligence, 2020.
Blodgett, S. L., Barocas, S., Daumé III, H., and Wallach, H. Language (technology) is power: A critical survey of “bias” in NLP. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pp. 5454-5476, Online, July 2020. Association for Com- putational Linguistics. doi: 10.18653/v1/2020.acl-main. 485. URL https://aclanthology.org/2020. acl-main.485."
2112.06905,2112.06905_29,GLaM Efficient Scaling of Language Models with Mixture-of-Experts.pdf,"Blodgett, S. L., Lopez, G., Olteanu, A., Sim, R., and Wallach, H. Stereotyping Norwegian salmon: An in- ventory of pitfalls in fairness benchmark datasets. In Proceedings of the 59th Annual Meeting of the Associa- tion for Computational Linguistics and the 11th Interna- tional Joint Conference on Natural Language Process- ing (Volume 1: Long Papers), pp. 1004-1015, Online, August 2021. Association for Computational Linguis- tics. doi: 10.18653/v1/2021.acl-long.81. URL https: //aclanthology.org/2021.acl-long.81.
Bender, E. M. and Friedman, B. Data statements for natural language processing: Toward mitigating system bias and enabling better science. Transactions of the Association for Computational Linguistics, 6:587-604, 2018. doi: 10. 1162/tacl_a_00041. URL https://aclanthology. org/Q18-1041.
Bolukbasi, T., Chang, K.-W., Zou, J. Y., Saligrama, V., and Kalai, A. T. Man is to computer programmer as woman is to homemaker? debiasing word em- beddings. In Lee, D., Sugiyama, M., Luxburg, U., Guyon, I., and Garnett, R. (eds.), Advances in Neural Information Processing Systems, volume 29. Curran As- sociates, Inc., 2016. URL https://proceedings. neurips.cc/paper/2016/file/
Bender, E. M., Gebru, T., McMillan-Major, A., and Shmitchell, S. On the dangers of stochastic parrots: Can language models be too big? In Proceedings of the 2021 ACM Conference on Fairness, Account- ability, and Transparency, FAccT ’21, pp. 610-623, New York, NY, USA, 2021. Association for Comput- ing Machinery. ISBN 9781450383097. doi: 10.1145/ 3442188.3445922. URL https://doi.org/10. 1145/3442188.3445922.
Berant, J., Chou, A., Frostig, R., and Liang, P. Semantic parsing on Freebase from question-answer pairs. In Pro- ceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pp. 1533-1544, Seattle, Washington, USA, October 2013. Association for Compu- tational Linguistics. URL https://aclanthology. org/D13-1160.
a486cd07e4ac3d270571622f4f316ec5-Paper. pdf."
2112.06905,2112.06905_30,GLaM Efficient Scaling of Language Models with Mixture-of-Experts.pdf,"a486cd07e4ac3d270571622f4f316ec5-Paper. pdf.
Bommasani, R., Hudson, D. A., Adeli, E., Altman, R., Arora, S., von Arx, S., Bernstein, M. S., Bohg, J., Bosse- lut, A., Brunskill, E., Brynjolfsson, E., Buch, S., Card, D., Castellon, R., Chatterji, N. S., Chen, A. S., Creel, K., Davis, J. Q., Demszky, D., Donahue, C., Doum- bouya, M., Durmus, E., Ermon, S., Etchemendy, J., Etha- yarajh, K., Fei-Fei, L., Finn, C., Gale, T., Gillespie, L., Goel, K., Goodman, N. D., Grossman, S., Guha, N., Hashimoto, T., Henderson, P., Hewitt, J., Ho, D. E., Hong, J., Hsu, K., Huang, J., Icard, T., Jain, S., Jurafsky, D., Kalluri, P., Karamcheti, S., Keeling, G., Khani, F., Khat- tab, O., Koh, P. W., Krass, M. S., Krishna, R., Kudi- tipudi, R., and et al. On the opportunities and risks of foundation models. CoRR, abs/2108.07258, 2021. URL https://arxiv.org/abs/2108.07258.
Bird, S. and Loper, E. NLTK: The natural language toolkit. In Proceedings of the ACL Interactive Poster and Demon- stration Sessions, pp. 214-217, Barcelona, Spain, July
Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry,
G., Askell, A., Agarwal, S., Herbert-Voss, A., Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler, D., Wu, J., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M., Gray, S., Chess, B., Clark, J., Berner, C., McCandlish, S., Radford, A., Sutskever, I., and Amodei, D. Language models are few-shot learners. In Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M. F., and Lin, H. (eds.), Advances in Neural Information Processing Systems, volume 33, pp. 1877-1901. Curran Asso- ciates, Inc., 2020. URL https://proceedings. neurips.cc/paper/2020/file/
1457cOd6ébfcb4967418bfb8acl42f64a—-Paper. pdf.
Caliskan, A., Bryson, J. J., and Narayanan, A. Seman- tics derived automatically from language corpora contain human-like biases. Science, 356(6334):183-186, Apr 2017. ISSN 1095-9203. doi: 10.1126/science.aal4230. URL http://dx.doi.org/10.1126/science. aal4230."
2112.06905,2112.06905_31,GLaM Efficient Scaling of Language Models with Mixture-of-Experts.pdf,"Carlini, N., Tramér, F., Wallace, E., Jagielski, M., Herbert- Voss, A., Lee, K., Roberts, A., Brown, T. B., Song, D., Er- lingsson, U., Oprea, A., and Raffel, C. Extracting training data from large language models. CoRR, abs/2012.07805, 2020.
Dagan, I., Glickman, O., and Magnini, B. The pascal recognising textual entailment challenge. In Quifionero- Candela, J., Dagan, I., Magnini, B., and d’Alché Buc, F (eds.), Machine Learning Challenges. Evaluating Pre- dictive Uncertainty, Visual Object Classification, and Recognising Tectual Entailment, pp. 177-190, Berlin, Heidelberg, 2006. Springer Berlin Heidelberg. ISBN 978-3-540-33428-6.
Dai, A. M. and Le, Q. V. Semi-supervised sequence learning. In Cortes, C., Lawrence, N., Lee, D., Sugiyama, M., and Garnett, R. (eds.), Advances in Neural Information Processing Systems, volume 28. Curran As- sociates, Inc., 2015. URL https://proceedings. neurips.cc/paper/2015/file/ 7137debd45ae4d0ab9aa953017286b20-Paper. pdf.
Dai, Z., Yang, Z., Yang, Y., Carbonell, J., Le, Q., and Salakhutdinov, R. Transformer-XL: Attentive language models beyond a fixed-length context. In Proceed- ings of the 57th Annual Meeting of the Association for Computational Linguistics, pp. 2978-2988, Florence, Italy, July 2019. Association for Computational Lin- guistics. doi: 10.18653/v1/P19-1285. URL https: //aclanthology.org/P19-1285.
Choi, E., He, H., lyyer, M., Yatskar, M., Yih, W.-t., Choi, Y., Liang, P., and Zettlemoyer, L. QuAC: Question an- swering in context. In Proceedings of the 2018 Confer- ence on Empirical Methods in Natural Language Pro- cessing, pp. 2174-2184, Brussels, Belgium, October- November 2018. Association for Computational Lin- guistics. doi: 10.18653/v1/D18-1241. URL https: //aclanthology.org/D18-1241."
2112.06905,2112.06905_32,GLaM Efficient Scaling of Language Models with Mixture-of-Experts.pdf,"Clark, C., Lee, K., Chang, M.-W., Kwiatkowski, T., Collins, M., and Toutanova, K. BoolQ: Exploring the surpris- ing difficulty of natural yes/no questions. In Proceed- ings of the 2019 Conference of the North American Chapter of the Association for Computational Linguis- tics: Human Language Technologies, Volume 1 (Long and Short Papers), pp. 2924-2936, Minneapolis, Min- nesota, June 2019. Association for Computational Lin- guistics. doi: 10.18653/v1/N19-1300. URL https: //aclanthology.org/N19-1300.
Dauphin, Y. N., Fan, A., Auli, M., and Grangier, D. Lan- guage modeling with gated convolutional networks. In International conference on machine learning, pp. 933- 941. PMLR, 2017.
de Marneffe, M.-C., Simons, M., and Tonhauser, J. The commitmentbank: Investigating projection in naturally occurring discourse. Proceedings of Sinn und Bedeutung, 23(2):107-124, Jul. 2019. doi: 10.18148/sub/2019.v23i2. 601. URL https://ojs.ub.uni-konstanz. de/sub/index.php/sub/article/view/601.
Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K. BERT: Pre-training of deep bidirectional transformers for lan- guage understanding. In Proceedings of the 2019 Confer- ence of the North American Chapter of the Association for Computational Linguistics: Human Language Tech- nologies, Volume I (Long and Short Papers), 2019.
Clark, K., Luong, M.-T., Le, Q. V., and Manning, C. D. Elec- tra: Pre-training text encoders as discriminators rather than generators. arXiv preprint arXiv:2003.10555, 2020.
Clark, P., Cowhey, I., Etzioni, O., Khot, T., Sabharwal, A., Schoenick, C., and Tafjord, O. Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv: 1803.05457v1, 2018."
2112.06905,2112.06905_33,GLaM Efficient Scaling of Language Models with Mixture-of-Experts.pdf,"Clark, P., Cowhey, I., Etzioni, O., Khot, T., Sabharwal, A., Schoenick, C., and Tafjord, O. Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv: 1803.05457v1, 2018.
Dua, D., Wang, Y., Dasigi, P., Stanovsky, G., Singh, S., and Gardner, M. DROP: A reading comprehension benchmark requiring discrete reasoning over paragraphs. In Burstein, J., Doran, C., and Solorio, T. (eds.), Pro- ceedings of the 2019 Conference of the North Ameri- can Chapter of the Association for Computational Lin- guistics: Human Language Technologies, NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Vol- ume I (Long and Short Papers), pp. 2368-2378. As- sociation for Computational Linguistics, 2019. doi:
10.18653/v1/n19-1246. URL https://doi.org/ 10.18653/v1/n19-1246.
Fedus, W., Zoph, B., and Shazeer, N. Switch transform- ers: Scaling to trillion parameter models with simple and efficient sparsity. CoRR, abs/2101.03961, 2021. URL https://arxiv.org/abs/2101.03961.
Hutchinson, B., Prabhakaran, V., Denton, E., Webster, K., Zhong, Y., and Denuyl, S. Social biases in NLP mod- els as barriers for persons with disabilities. In Pro- ceedings of the 58th Annual Meeting of the Associa- tion for Computational Linguistics, pp. 5491-5501, On- line, July 2020. Association for Computational Linguis- tics. doi: 10.18653/v1/2020.acl-main.487. URL https: //aclanthology.org/2020.acl-main. 487.
Fyodorov, Y., Winter, Y., and Francez, N. A natural logic in- ference system. In Inference in Computational Semantics, 2000.
Gehman, S., Gururangan, S., Sap, M., Choi, Y., and Smith, N. A. Realtoxicityprompts: Evaluating neural toxic de- generation in language models, 2020.
Jacobs, A. Z. and Wallach, H. Measurement and fair- ness. Proceedings of the 2021 ACM Conference on Fair- ness, Accountability, and Transparency, Mar 2021. doi: 10.1145/3442188.3445901. URL http://dx.doi. org/10.1145/3442188.3445901."
2112.06905,2112.06905_34,GLaM Efficient Scaling of Language Models with Mixture-of-Experts.pdf,"Jacobs, A. Z. and Wallach, H. Measurement and fair- ness. Proceedings of the 2021 ACM Conference on Fair- ness, Accountability, and Transparency, Mar 2021. doi: 10.1145/3442188.3445901. URL http://dx.doi. org/10.1145/3442188.3445901.
Gordon, A., Kozareva, Z., and Roemmele, M. SemEval- 2012 task 7: Choice of plausible alternatives: An evalua- tion of commonsense causal reasoning. In *SEM 20/2: The First Joint Conference on Lexical and Computational Semantics — Volume 1: Proceedings of the main confer- ence and the shared task, and Volume 2: Proceedings of the Sixth International Workshop on Semantic Evaluation (SemEval 2012), pp. 394-398, Montréal, Canada, 7-8 June 2012. Association for Computational Linguistics. URL https://aclanthology.org/S12-1052.
Joshi, M., Choi, E., Weld, D. S., and Zettlemoyer, L. Trivi- aqa: A large scale distantly supervised challenge dataset for reading comprehension. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, Vancouver, Canada, July 2017. Association for Computational Linguistics.
Kaplan, J., McCandlish, S., Henighan, T., Brown, T. B., Chess, B., Child, R., Gray, S., Radford, A., Wu, J., and Amodei, D. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020.
Hendrycks, D. and Gimpel, K. Bridging nonlinearities and stochastic regularizers with gaussian error linear units. CoRR, abs/1606.08415, 2016. URL http: //arxiv. org/abs/1606.08415.
Hestness, J., Narang, S., Ardalani, N., Diamos, G. F., Jun, H., Kianinejad, H., Patwary, M. M. A., Yang, Y., and Zhou, Y. Deep learning scaling is predictable, empirically. CoRR, abs/1712.00409, 2017. URL http: //arxiv. org/abs/1712.00409."
2112.06905,2112.06905_35,GLaM Efficient Scaling of Language Models with Mixture-of-Experts.pdf,"Hestness, J., Narang, S., Ardalani, N., Diamos, G. F., Jun, H., Kianinejad, H., Patwary, M. M. A., Yang, Y., and Zhou, Y. Deep learning scaling is predictable, empirically. CoRR, abs/1712.00409, 2017. URL http: //arxiv. org/abs/1712.00409.
Khashabi, D., Chaturvedi, S., Roth, M., Upadhyay, S., and Roth, D. Looking beyond the surface: A chal- lenge set for reading comprehension over multiple sen- tences. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Com- putational Linguistics: Human Language Technologies, Volume I (Long Papers), pp. 252-262, New Orleans, Louisiana, June 2018. Association for Computational Linguistics. doi: 10.18653/v1/N18-1023. URL https: //aclanthology.org/N18-1023.
Houlsby, N., Giurgiu, A., Jastrzebski, S., Morrone, B., De Laroussilhe, Q., Gesmundo, A., Attariyan, M., and Gelly, S. Parameter-efficient transfer learning for NLP. In Chaudhuri, K. and Salakhutdinov, R. (eds.), Proceed- ings of the 36th International Conference on Machine Learning, volume 97 of Proceedings of Machine Learn- ing Research, pp. 2790-2799. PMLR, 09-15 Jun 2019. URL https://proceedings.mlr.press/v97/ houlsby19a.html.
Kiros, R., Zhu, Y., Salakhutdinov, R. R., Zemel, R., Urtasun, R., Torralba, A., and Fidler, S. Skip-thought vectors. In Cortes, C., Lawrence, N., Lee, D., Sugiyama, M., and Garnett, R. (eds.), Advances in Neural Information Processing Systems, volume 28. Curran As- sociates, Inc., 2015. URL https://proceedings. neurips.cc/paper/2015/file/ £442d33£a06832082290ad8544a8da27-Paper.
£442d33£a06832082290ad8544a8da27-Paper. pdf."
2112.06905,2112.06905_36,GLaM Efficient Scaling of Language Models with Mixture-of-Experts.pdf,"£442d33£a06832082290ad8544a8da27-Paper. pdf.
Huang, Y., Cheng, Y., Bapna, A., Firat, O., Chen, D., Chen, M. X., Lee, H., Ngiam, J., Le, Q. V., Wu, Y., and Chen, Z. Gpipe: Efficient training of giant neural networks using pipeline parallelism. In Wallach, H. M., Larochelle, H., Beygelzimer, A., d’ Alché-Buc, F., Fox, E. B., and Garnett, R. (eds.), Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, pp. 103-112, 2019.
Kudo, T. and Richardson, J. Sentencepiece: A simple and language independent subword tokenizer and detokenizer for neural text processing. In EMNLP, 2018.
Kudugunta, S., Huang, Y., Bapna, A., Krikun, M., Lep- ikhin, D., Luong, M.-T., and Firat, O. Beyond distillation: Task-level mixture-of-experts for efficient inference. In Findings of the Association for Computational Linguis- tics: EMNLP 2021, pp. 3577-3599, 2021.
Kwiatkowski, T., Palomaki, J., Redfield, O., Collins, M., Parikh, A., Alberti, C., Epstein, D., Polosukhin, L., Kel- cey, M., Devlin, J., Lee, K., Toutanova, K. N., Jones, L., Chang, M.-W., Dai, A., Uszkoreit, J., Le, Q., and Petrov, S. Natural questions: a benchmark for question answering research. Transactions of the Association of Computational Linguistics, 2019.
Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.findings-emnlp. 311. URL https://aclanthology.org/2020. findings-emnlp.311.
Lieber, O., Sharir, O., Lenz, B., and Shoham, Y. Jurassic-1: Technical details and evaluation. White Paper. AI21 Labs, 2021.
Lai, G., Xie, Q., Liu, H., Yang, Y., and Hovy, E. RACE: Large-scale ReAding comprehension dataset from ex- aminations. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pp. 785-794, Copenhagen, Denmark, September 2017. Asso- ciation for Computational Linguistics. doi: 10.18653/v1/ D17-1082. URL https://aclanthology.org/ D17-1082."
2112.06905,2112.06905_37,GLaM Efficient Scaling of Language Models with Mixture-of-Experts.pdf,"Lamm, M., Palomaki, J., Alberti, C., Andor, D., Choi, E., Soares, L. B., and Collins, M. QED: A framework and dataset for explanations in question answering. CoRR, abs/2009.06354, 2020. URL https: //arxiv.org/ abs/2009.06354.
Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M., Zettlemoyer, L., and Stoyanov, V. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692, 2019.
May, C., Wang, A., Bordia, S., Bowman, S. R., and Rudinger, R. On measuring social biases in sentence encoders. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Com- putational Linguistics: Human Language Technologies, Volume I (Long and Short Papers), pp. 622-628, Min- neapolis, Minnesota, June 2019. Association for Compu- tational Linguistics. doi: 10.18653/v1/N19-1063. URL https: //aclanthology.org/N19-1063.
Le, Q. and Mikolov, T. Distributed representations of sen- tences and documents. In International conference on machine learning, 2014.
Mihaylov, T., Clark, P., Khot, T., and Sabharwal, A. Can a suit of armor conduct electricity? a new dataset for open book question answering. In EMNLP, 2018.
Leidner, J. L. and Plachouras, V. Ethical by design: Ethics best practices for natural language processing. In Proceedings of the First ACL Workshop on Ethics in Natural Language Processing, pp. 30-40, Valencia, Spain, April 2017. Association for Computational Lin- guistics. doi: 10.18653/v1/W17-1604. URL https: //aclanthology.org/W17-1604.
Lepikhin, D., Lee, H., Xu, Y., Chen, D., Firat, O., Huang, Y., Krikun, M., Shazeer, N., and Chen, Z. GShard: Scal- ing giant models with conditional computation and auto- matic sharding. In International Conference on Learning Representations, 2021. URL https: //openreview. net /forum?id=qrwe7XHTmYb."
2112.06905,2112.06905_38,GLaM Efficient Scaling of Language Models with Mixture-of-Experts.pdf,"Levesque, H., Davis, E., and Morgenstern, L. The wino- grad schema challenge. In 13th International Confer- ence on the Principles of Knowledge Representation and Reasoning, KR 2012, Proceedings of the International Conference on Knowledge Representation and Reason- ing, pp. 552-561. Institute of Electrical and Electronics Engineers Inc., 2012. ISBN 9781577355601. 13th In- ternational Conference on the Principles of Knowledge Representation and Reasoning, KR 2012 ; Conference date: 10-06-2012 Through 14-06-2012.
Li, T., Khashabi, D., Khot, T., Sabharwal, A., and Srikumar, V. UNQOVERing stereotyping biases via underspeci- fied questions. In Findings of the Association for Com- putational Linguistics: EMNLP 2020, pp. 3475-3489,
Mikoloy, T., Karafiat, M., Burget, L., Cernocky, J. H., and Khudanpur, S. Recurrent neural network based language model. In INTERSPEECH, 2010.
Mikolov, T., Chen, K., Corrado, G., and Dean, J. Efficient estimation of word representations in vector space. In Bengio, Y. and LeCun, Y. (eds.), /st International Confer- ence on Learning Representations, ICLR 2013, Scottsdale, Arizona, USA, May 2-4, 2013, Workshop Track Proceed- ings, 2013. URL http: //arxiv.org/abs/1301. 3781.
Mostafazadeh, N., Chambers, N., He, X., Parikh, D., Ba- tra, D., Vanderwende, L., Kohli, P., and Allen, J. A corpus and cloze evaluation for deeper understanding of commonsense stories. In Proceedings of the 2016 Conference of the North American Chapter of the As- sociation for Computational Linguistics: Human Lan- guage Technologies, pp. 839-849, San Diego, Cali- fornia, June 2016. Association for Computational Lin- guistics. doi: 10.18653/v1/N16-1098. URL https: //aclanthology.org/N16-1098.
Nadeem, M., Bethke, A., and Reddy, S. StereoSet: Mea- suring stereotypical bias in pretrained language models. In Proceedings of the 59th Annual Meeting of the As- sociation for Computational Linguistics and the 11th International Joint Conference on Natural Language Pro- cessing (Volume 1: Long Papers), pp. 5356-5371, Online,"
2112.06905,2112.06905_39,GLaM Efficient Scaling of Language Models with Mixture-of-Experts.pdf,"August 2021. Association for Computational Linguis- tics. doi: 10.18653/v1/2021.acl-long.416. URL https: //aclanthology.org/2021.acl-long. 416.
Paperno, D., Kruszewski, G., Lazaridou, A., Pham, N. Q., Bernardi, R., Pezzelle, S., Baroni, M., Boleda, G., and Fernandez, R. The LAMBADA dataset: Word prediction requiring a broad discourse context. In Proceedings of the 54th Annual Meeting of the Association for Compu- tational Linguistics (Volume 1: Long Papers), pp. 1525- 1534, Berlin, Germany, August 2016. Association for Computational Linguistics. doi: 10.18653/v1/P16-1144. URL https://aclanthology.org/P16-1144.
A., de Las Casas, D., Guy, A., Jones, C., Bradbury, J., Johnson, M., Hechtman, B. A., Weidinger, L., Gabriel, IL, Isaac, W. S., Lockhart, E., Osindero, S., Rimell, L., Dyer, C., Vinyals, O., Ayoub, K., Stanway, J., Bennett, L., Hassabis, D., Kavukcuoglu, K., and Irving, G. Scal- ing language models: Methods, analysis & insights from training gopher. CoRR, abs/2112.11446, 2021.
Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., and Liu, P. J. Exploring the limits of transfer learning with a unified text-to-text trans- former. J. Mach. Learn. Res., 21:140:1-140:67, 2020. URL http://jmlr.org/papers/v21/20-074. html.
Patterson, D., Gonzalez, J., Le, Q., Liang, C., Munguia, L.- M., Rothchild, D., So, D., Texier, M., and Dean, J. Car- bon emissions and large neural network training. arXiv preprint arXiv:2104. 10350, 2021.
Rajpurkar, P., Jia, R., and Liang, P. Know what you don’t know: Unanswerable questions for squad. In ACL, 2018.
Pennington, J., Socher, R., and Manning, C. GloVe: Global vectors for word representation. In Proceedings of the 2014 Conference on Empirical Methods in Nat- ural Language Processing (EMNLP), pp. 1532-1543, Doha, Qatar, October 2014. Association for Computa- tional Linguistics. doi: 10.3115/v1/D14-1162. URL https: //aclanthology.org/D14-1162."
2112.06905,2112.06905_40,GLaM Efficient Scaling of Language Models with Mixture-of-Experts.pdf,"Peters, M. E., Neumann, M., Iyyer, M., Gardner, M., Clark, C., Lee, K., and Zettlemoyer, L. Deep contextualized word representations. arXiv preprint arXiv: 1802.05365, 2018.
Pilehvar, M. T. and Camacho-Collados, J. Wic: 10, 000 example pairs for evaluating context-sensitive representa- tions. ArXiv, abs/1808.09121, 2018.
Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., and Sutskever, I. Language models are unsupervised multitask learners. 2018. URL https://d4mucfpksywv.cloudfront. net/better-language-models/ language-models.pdf.
Reddy, S., Chen, D., and Manning, C. D. CoQA: A con- versational question answering challenge. Transactions of the Association for Computational Linguistics, 7:249- 266, March 2019. doi: 10.1162/tacl_a_00266. URL https://aclanthology.org/Q19-1016.
Rogers, A. Changing the world by changing the data. In Proceedings of the 59th Annual Meeting of the Associa- tion for Computational Linguistics and the 11th Interna- tional Joint Conference on Natural Language Process- ing (Volume 1: Long Papers), pp. 2182-2194, Online, August 2021. Association for Computational Linguis- tics. doi: 10.18653/v1/2021.acl-long.170. URL https: //aclanthology.org/2021.acl-long.170.
Rudinger, R., May, C., and Van Durme, B. Social bias in elicited natural language inferences. In Proceedings of the First ACL Workshop on Ethics in Natural Language Processing, pp. 74-79, Valencia, Spain, April 2017. Asso- ciation for Computational Linguistics. doi: 10.18653/v1/ W17-1609. URL https://aclanthology.org/ W17-1609."
2112.06905,2112.06905_41,GLaM Efficient Scaling of Language Models with Mixture-of-Experts.pdf,"Rae, J. W., Borgeaud, S., Cai, T., Millican, K., Hoffmann, J., Song, H. F., Aslanides, J., Henderson, S., Ring, R., Young, S., Rutherford, E., Hennigan, T., Menick, J., Cassirer, A., Powell, R., van den Driessche, G., Hendricks, L. A., Rauh, M., Huang, P., Glaese, A., Welbl, J., Dathathri, S., Huang, S., Uesato, J., Mellor, J., Higgins, I., Creswell, A., McAleese, N., Wu, A., Elsen, E., Jayakumar, S. M., Buchatskaya, E., Budden, D., Sutherland, E., Simonyan, K., Paganini, M., Sifre, L., Martens, L., Li, X. L., Kun- coro, A., Nematzadeh, A., Gribovskaya, E., Donato, D., Lazaridou, A., Mensch, A., Lespiau, J., Tsimpoukelli, M.., Grigorev, N., Fritz, D., Sottiaux, T., Pajarskas, M., Pohlen, T., Gong, Z., Toyama, D., de Masson d’ Autume, C., Li, Y., Terzi, T., Mikulik, V., Babuschkin, I., Clark,
Rudinger, R., Naradowsky, J., Leonard, B., and Van Durme, B. Gender bias in coreference resolution. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers), pp. 8— 14, New Orleans, Louisiana, June 2018. Association for Computational Linguistics. doi: 10.18653/v1/N18-2002. URL https://aclanthology.org/N18-2002.
Sakaguchi, K., Bras, R. L., Bhagavatula, C., and Choi, Y. Winogrande: An adversarial winograd schema challenge at scale. In AAAI, pp. 8732-8740. AAAI Press, 2020.
Sap, M., Gabriel, S., Qin, L., Jurafsky, D., Smith, N. A., and Choi, Y. Social bias frames: Reasoning about
social and power implications of language. In Pro- ceedings of the 58th Annual Meeting of the Associa- tion for Computational Linguistics, pp. 5477-5490, On- line, July 2020. Association for Computational Linguis- tics. doi: 10.18653/v1/2020.acl-main.486. URL https: //aclanthology.org/2020.acl-main. 486.
Shazeer, N. Glu variants improve transformer, 2020."
2112.06905,2112.06905_42,GLaM Efficient Scaling of Language Models with Mixture-of-Experts.pdf,"Shazeer, N. Glu variants improve transformer, 2020.
Sotnikova, A., Cao, Y. T., Daumé II, H., and Rudinger, R. Analyzing stereotypes in generative text infer- ence tasks. In Findings of the Association for Com- putational Linguistics: ACL-IJCNLP 2021, pp. 4052- 4065, Online, August 2021. Association for Computa- tional Linguistics. doi: 10.18653/v1/2021.findings-acl. 355. URL https://aclanthology.org/2021. findings-acl.355.
Shazeer, N. and Stern, M. Adafactor: Adaptive learning rates with sublinear memory cost. ArXiv, abs/1804.04235, 2018.
Shazeer, N., Mirhoseini, A., Maziarz, K., Davis, A., Le, Q. V., Hinton, G. E., and Dean, J. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. In Sth International Conference on Learning Rep- resentations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings. OpenReview.net, 2017. URL https: //openreview.net/forum? id=BlckMDqlg.
Shazeer, N., Cheng, Y., Parmar, N., Tran, D., Vaswani, A., Koanantakool, P., Hawkins, P., Lee, H., Hong, M., Young, C., Sepassi, R., and Hechtman, B. Mesh-tensorflow: Deep learning for supercomputers. In Proceedings of the 32nd International Conference on Neural Information Processing Systems, NIPS’18, pp. 10435-10444, Red Hook, NY, USA, 2018. Curran Associates Inc."
2112.06905,2112.06905_43,GLaM Efficient Scaling of Language Models with Mixture-of-Experts.pdf,"Shen, J., Nguyen, P., Wu, Y., Chen, Z., Chen, M. X., Jia, Y., Kannan, A., Sainath, T. N., Cao, Y., Chiu, C., He, Y., Chorowski, J., Hinsu, S., Laurenzo, S., Qin, J., Firat, O., Macherey, W., Gupta, S., Bapna, A., Zhang, S., Pang, R., Weiss, R. J., Prabhavalkar, R., Liang, Q., Jacob, B., Liang, B., Lee, H., Chelba, C., Jean, S., Li, B., Johnson, M., Anil, R., Tibrewal, R., Liu, X., Eriguchi, A., Jaitly, N., Ari, N., Cherry, C., Haghani, P., Good, O., Cheng, Y., Alvarez, R., Caswell, I., Hsu, W., Yang, Z., Wang, K., Gonina, E., Tomanek, K., Vanik, B., Wu, Z., Jones, L., Schuster, M., Huang, Y., Chen, D., Irie, K., Foster, G. F, Richardson, J., Macherey, K., Bruguier, A., Zen, H., Raffel, C., Kumar, S., Rao, K., Rybach, D., Murray, M., Peddinti, V., Krikun, M., Bacchiani, M., Jablin, T. B., Suderman, R., Williams, I., Lee, B., Bhatia, D., Carlson, J., Yavuz, S., Zhang, Y., McGraw, L., Galkin, M., Ge, Q., Pundak, G., Whipkey, C., Wang, T., Alon, U., Lepikhin, D., Tian, Y., Sabour, S., Chan, W., Toshniwal, S., Liao, B., Nirschl, M., and Rondon, P. Lingvo: a modular and scalable framework for sequence-to-sequence modeling. CoRR, abs/1902.08295, 2019.
Stanovsky, G., Smith, N. A., and Zettlemoyer, L. Eval- uating gender bias in machine translation. In Proceed- ings of the 57th Annual Meeting of the Association for Computational Linguistics, pp. 1679-1684, Florence, Italy, July 2019. Association for Computational Lin- guistics. doi: 10.18653/v1/P19-1164. URL https: //aclanthology.org/P19-1164.
Strubell, E., Ganesh, A., and McCallum, A. Energy and policy considerations for deep learning in NLP. In Proceedings of the 57th Annual Meeting of the Asso- ciation for Computational Linguistics, pp. 3645-3650, Florence, Italy, July 2019. Association for Computa- tional Linguistics. doi: 10.18653/v1/P19-1355. URL https://aclanthology.org/P19-1355."
2112.06905,2112.06905_44,GLaM Efficient Scaling of Language Models with Mixture-of-Experts.pdf,"Sutskever, I., Martens, J., and Hinton, G. Generating text with recurrent neural networks. In Proceedings of the 28th International Conference on International Conference on Machine Learning, CML 11, pp. 1017-1024, Madison, WI, USA, 2011. Omnipress. ISBN 9781450306195.
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L. u., and Polosukhin, I. Attention is all you need. In Guyon, I, Luxburg, U. V., Bengio, S., Wallach, H., Fergus, R., Vish- wanathan, S., and Garnett, R. (eds.), Advances in Neural Information Processing Systems, volume 30. Curran As- sociates, Inc., 2017. URL https://proceedings. neurips.cc/paper/2017/file/
3£5ee243547dee91 fbd053clc4a845aa-Paper. pdf.
Wang, A., Pruksachatkun, Y., Nangia, N., Singh, A., Michael, J., Hill, F, Levy, O., and Bowman, S. Su- perglue: A stickier benchmark for general-purpose language understanding systems. In Wallach, H., Larochelle, H., Beygelzimer, A., d’Alché Buc, F, Fox, E., and Garnett, R. (eds.), Advances in Neural Information Processing Systems, volume 32. Curran As- sociates, Inc., 2019. URL https://proceedings. neurips.cc/paper/2019/file/ 4496bf24afe7fabb6f046bf4923da8de6—-Paper.
Shoeybi, M., Patwary, M., Puri, R., LeGresley, P., Casper, J., and Catanzaro, B. Megatron-Im: Training multi-billion parameter language models using gpu model parallelism. arXiv preprint arXiv: 1909.08053, 2019.
4496bf24afe7fabb6f046bf4923da8de6—-Paper. pdf.
Webster, K. and Pitler, E. ots to model pronoun gender for translation. Scalable cross lingual piv- CoRR,
abs/2006.08881, 2020. URL https: //arxiv.org/ abs/2006.08881.
Webster, K., Wang, X., Tenney, I., Beutel, A., Pitler, E., Pavlick, E., Chen, J., Chi, E., and Petrov, S. Measuring and reducing gendered correlations in pre-trained models, 2021.
Wei, J., Bosma, M., Zhao, V. Y., Guu, K., Yu, A. W., Lester, B., Du, N., Dai, A. M., and Le, Q. V. Finetuned language models are zero-shot learners, 2021."
2112.06905,2112.06905_45,GLaM Efficient Scaling of Language Models with Mixture-of-Experts.pdf,"Wei, J., Bosma, M., Zhao, V. Y., Guu, K., Yu, A. W., Lester, B., Du, N., Dai, A. M., and Le, Q. V. Finetuned language models are zero-shot learners, 2021.
Welbl, J., Glaese, A., Uesato, J., Dathathri, S., Mel- lor, J., Hendricks, L. A., Anderson, K., Kohli, P., Coppin, B., and Huang, P.-S. Challenges in detox- ifying language models. In Findings of the As- sociation for Computational Linguistics: _EMNLP 2021, pp. 2447-2469, Punta Cana, Dominican Repub- lic, November 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.findings-emnlp. 210. URL https://aclanthology.org/2021. findings-emnlp.210.
the Association for Computational Linguistics, pp. 4791- 4800, Florence, Italy, July 2019. Association for Compu- tational Linguistics. doi: 10.18653/v1/P19-1472. URL https://aclanthology.org/P19-1472.
Zhang, S., Liu, X., Liu, J., Gao, J., Duh, K., and Durme, B. V. Record: Bridging the gap between human and machine commonsense reading comprehension. CoRR, abs/1810.12885, 2018.
Zhao, J., Wang, T., Yatskar, M., Ordonez, V., and Chang, K.-W. Gender bias in coreference resolution: Evalua- tion and debiasing methods. In Proceedings of the 2018 Conference of the North American Chapter of the Associ- ation for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers), pp. 15-20, New Orleans, Louisiana, June 2018. Association for Compu- tational Linguistics. doi: 10.18653/v1/N18-2003. URL https: //aclanthology.org/N18-2003.
Xu, Y., Lee, H., Chen, D., Hechtman, B. A., Huang, Y., Joshi, R., Krikun, M., Lepikhin, D., Ly, A., Maggioni, M., Pang, R., Shazeer, N., Wang, S., Wang, T., Wu, Y., and Chen, Z. GSPMD: general and scalable parallelization for ML computation graphs. CoRR, abs/2105.04663, 2021. URL https://arxiv.org/abs/2105.04663.
Yang, Z., Dai, Z., Yang, Y., Carbonell, J., Salakhutdinov, R.R., and Le, Q. V. XInet: Generalized autoregressive pretraining for language understanding. Advances in neural information processing systems, 32, 2019."
2112.06905,2112.06905_46,GLaM Efficient Scaling of Language Models with Mixture-of-Experts.pdf,"Yang, Z., Dai, Z., Yang, Y., Carbonell, J., Salakhutdinov, R.R., and Le, Q. V. XInet: Generalized autoregressive pretraining for language understanding. Advances in neural information processing systems, 32, 2019.
Yu, D., Zhu, C., Fang, Y., Yu, W., Wang, S., Xu, Y., Ren, X., Yang, Y., and Zeng, M. KG-FiD: Infusing knowledge graph in fusion-in-decoder for open-domain question answering. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 4961-4974, Dublin, Ire- land, May 2022. Association for Computational Linguis- tics. doi: 10.18653/v1/2022.acl-long.340. URL https: //aclanthology.org/2022.acl-long.340.
Yu, Y., Abadi, M., Barham, P., Brevdo, E., Burrows, M., Davis, A., Dean, J., Ghemawat, S., Harley, T., Hawkins, P., Isard, M., Kudlur, M., Monga, R., Murray, D., and Zheng, X. Dynamic control flow in large-scale machine learning. In Proceedings of the Thirteenth EuroSys Con- ference, EuroSys ’18, New York, NY, USA, 2018. Associ- ation for Computing Machinery. ISBN 9781450355841. doi: 10.1145/3190508.3190551. URL https: //doi. org/10.1145/3190508.3190551.
Zellers, R., Holtzman, A., Bisk, Y., Farhadi, A., and Choi, Y. HellaSwag: Can a machine really finish your sen- tence? In Proceedings of the 57th Annual Meeting of"
2112.06905,2112.06905_47,GLaM Efficient Scaling of Language Models with Mixture-of-Experts.pdf,"A. Benchmarks
Open-Domain Question Answering: TriviaQA (Joshi et al., 2017), Natural Questions (NQS) (Kwiatkowski et al., 2019), Web Questions (WebQS) (Berant et al., 2013)
Cloze and Completion Tasks: LAMBADA (Paperno et al., 2016), HellaSwag (Zellers et al., 2019), StoryCloze (Mostafazadeh et al., 2016)
Winograd-Style Tasks: Winograd (Levesque et al., 2012), WinoGrande (Sakaguchi et al., 2020)
Common Sense Reasoning: PIQA (Bisk et al., 2020), ARC (Easy) (Clark et al., 2018), ARC (Chal- lenge) (Clark et al., 2018), OpenBookQA (Mihaylov et al., 2018)
In-context Reading Comprehension: DROP (Dua et al., 2019), CoQA (Reddy et al., 2019), QUAC (Choi et al., 2018), SQuADv2 (Rajpurkar et al., 2018), RACE- h (Lai et al., 2017), RACE-m (Lai et al., 2017)
SuperGLUE: (Wang et al., 2019) BoolQ (Clark et al., 2019), CB (de Marneffe et al., 2019), COPA (Gordon et al., 2012), RTE (Dagan et al., 2006), WiC (Pile- hvar & Camacho-Collados, 2018), WSC (Levesque et al., 2012), MultiRC (Khashabi et al., 2018), ReCoRD (Zhang et al., 2018)
Natural Language Inference: ANLI RI, ANLI R3 (Fyodorov et al., 2000) ANLI_ R2,"
2112.06905,2112.06905_48,GLaM Efficient Scaling of Language Models with Mixture-of-Experts.pdf,"B. Scaling the Number of Experts
We also study the effects of increasing the number of experts per MoE layer. More concretely, we start with a modest size model of 1.7B, which essentially is a GLaM (1.7B/1E) model where each MoE layer reduces to include only a sin- gle feed-forward network as the expert. We then increase the number of experts in each MoE layer from 1 to 256. Despite the fact that the number of experts increases expo- nentially, the Mact-params In each model barely increases due to the sparsity of GLaM. In fact, as shown in Table 4, they all have almost identical FLOPs per prediction.
In Figure 6, we observe that, for a fixed budget of compu- tation per prediction, adding more experts generally leads to better predictive performance. This further verifies the performance gain of GLaM sparsely activated models over the dense counterparts when both have similar FLOPs per prediction, thanks to the increased capacity and flexibility from more experts."
2112.06905,2112.06905_49,GLaM Efficient Scaling of Language Models with Mixture-of-Experts.pdf,"C. Model Partitioning
We partition the weights and computation of large GLaM models using the 2D sharding algorithm as described in Xu et al. (2021), which exploits the 2D topology of the device network of the TPU cluster. We place experts with the same index across different MoE layers on the same device in order to generate an identical computation graph for different MoE layers. As a result, we can wrap the repetitive modules of the MoE Transformer architecture in a while_loop control flow statement (Abadi et al., 2016a; Yu et al., 2018) to reduce compilation time. Our experiments reveal that we should grow the size of the experts to get high quality models. Therefore, when each expert gets sufficiently large, we have to allocate each expert across a set of x devices. For example, we partition the expert weight tensor with the shape [E’, /, H] in the MoE layer along the expert dimension FE, and hidden dimension H, and partition the input activation tensors with the shape [B, S, /] along the batch dimension B and the model dimension M. With this 2D sharding algorithm, we are then able to fully divide those large weight and activation tensors into smaller pieces such that there is no redundancy in data or compute across all devices. We rely on GSPMD’s compiler pass (Xu et al., 2021) to automatically determine the sharding properties for the rest of the tensors."
2112.06905,2112.06905_50,GLaM Efficient Scaling of Language Models with Mixture-of-Experts.pdf,"D. Data Contamination
As GLaM was trained on over 1.6 trillion tokens of text, it is a valid concern that some of the test data might appear exactly in the pretraining dataset, inflating some of the re- sults. We therefore follow Brown et al. (2020) and Wei et al. (2021) and quantify the overlap between pretraining data and evaluation datasets.
Our analysis uses the same methodology as Wei et al. (2021), which, in turn closely follows Brown et al. (2020). For each evaluation dataset we report the number of examples which overlap with the pretraining data, defining overlap as
Table 6. Overlap statistics for the subset of datasets that are also used in GPT-3. An evaluation example was dirty if it had any n-gram collision with the pretraining corpus.
Table 7. Prompts used for each co-occurrence category.
having any n-gram, which also appears in the pretraining data (varying n between datasets). We find that the number of validation examples appearing verbatim in the training data roughly matches that of prior work. We report these numbers in Table 6.
for low toxicity prompts in Figure 8. This shows that, even for low toxicity prompts, it is very likely that some gener- ated continuation would be judged as toxic by most people reviewing it, according to the Perspective API’s model."
2112.06905,2112.06905_51,GLaM Efficient Scaling of Language Models with Mixture-of-Experts.pdf,"E. Ethics and Unintended Biases
Like Rae et al. (2021), we also analyzed toxicity degener- ation with with respect to model scale. This is shown in Figure 7. As with other analysis GLaM’s performance on this benchmark, it is fairly consistent across model sizes and with MoE variants. The 0.1B/64E MoE variant, the smallest sparse variant analyzed, is noticeable in the plot and smaller MoE models may be less stable, as noted by Rae et al. (2021).
Following Rae et al. (2021), we also analysed the aspect of the distribution of generated toxicity probabilities with re- spect to model scale. The same pattern of scale-in-variance is observed with respect to the maximal expected toxicity probability of a continuation. The distribution of toxicity probabilities from the API for 25 continuations is plotted
Table 7 shows the prompts used for the co-occurrence eval- uation; these are the same as those of Brown et al. (2020). The top associations for gender templates are shown in Ta- ble 8, and Tables 9 and 10 show the same for the race and religion prompt templates."
2112.06905,2112.06905_52,GLaM Efficient Scaling of Language Models with Mixture-of-Experts.pdf,"F. Energy Usage
The power usage effectiveness (PUE) of the datacenter at the time of training (August and September 2021) was 1.11. Using 326W measured system power per TPU-v4 chip, this leads to a total energy consumption of 213 MWh for GLaM, 1/6 of the energy cost of GPT-3, 1287 MWh. The datacenter PUE was 1.10 at the time of training GPT-3 (Patterson et al., 2021). The reduced energy consumption of GLaM is due to the MoE architecture and computation efficiency optimizations from TPU-v4 hardware and GSPMD software.
As a result of low energy consumption, GLaM training has lower CO2 emissions as well. The net tCOze per MWh of the datacenter at the time was 0.088, training GLaM with 280B tokens emits a total of 18.7 net tCOze, compared to 552 net tCO2e for GPT-3 (Patterson et al., 2021). The complete GLaM training using 600B tokens consumes only 456 MWh and emits 40.2 net tCOze.
Table 8. Gender: top co-occurrences for prompts like “{term} was very...”
Table 9. Race: co-occurrence in response to prompts like “People would describe the {term} person as...”.

G. Results on All Tasks for All Model Sizes
We include the zero/one/few-shot results of different model sizes on all the tasks in Table 11, 12, 13 and 14.
Table 10. Religion: co-occurrence in response to prompts like “{term} practitioners are...”

One-shot

Zero-shot

Few-shot (shots)

Avg NLU

Avg NLU

GLaM (MoE)

GLaM (Dense)

GPT3

Avg NLU"
2112.06905,2112.06905_53,GLaM Efficient Scaling of Language Models with Mixture-of-Experts.pdf,"<table><caption>Table 3. Data and mixture weights in GLaM training set.</caption><thead><tr><th></th><th></th><th>GPT-3</th><th>GLaM_</th><th>relative</th></tr></thead><tbody><tr><td rowspan=""2"">cost</td><td>FLOPs / token (G)</td><td>350</td><td>180</td><td>-48.6%</td></tr><tr><td>Train energy (MWh)</td><td>1287</td><td>456</td><td>-64.6%</td></tr><tr><td></td><td>Zero-shot</td><td>56.9</td><td>62.7</td><td>+10.2%</td></tr><tr><td>oe accuracy</td><td>One-shot</td><td>616</td><td>65.5</td><td>+6.3%</td></tr><tr><td>on average</td><td>Few-shot</td><td>65.2</td><td>681</td><td>+4.4%</td></tr></tbody></table>"
2112.06905,2112.06905_54,GLaM Efficient Scaling of Language Models with Mixture-of-Experts.pdf,<table><caption>Table 4. Sizes and architectures of both MoE and dense models that we have trained in our experiments. Models are grouped by the number of activated parameters per token. All trained models share the same learning hyperparameters described in Session 5.1.</caption><thead><tr><th>Model Name</th><th>Model Type</th><th>Nparams</th><th>— Nact-paran</th></tr></thead><tbody><tr><td>BERT</td><td>Dense Encoder-only</td><td>340M</td><td>340M</td></tr><tr><td>TS</td><td>Dense Encoder-decoder</td><td>13B</td><td>13B</td></tr><tr><td>GPT-3</td><td>Dense Decoder-only</td><td>175B.-175B</td><td></td></tr><tr><td>Jurassic-1</td><td>Dense Decoder-only</td><td>178B—-178B</td><td></td></tr><tr><td>Gopher</td><td>Dense Decoder-only</td><td>280B</td><td>280B</td></tr><tr><td>Megatron-530B</td><td>Dense Decoder-only</td><td>530B</td><td>530B</td></tr><tr><td>GShard-M4</td><td>MoE Encoder-decoder</td><td>600B</td><td>1.5B</td></tr><tr><td>Switch-C</td><td>MoE Encoder-decoder</td><td>1.5T</td><td>1.5B</td></tr><tr><td>GLaM (64B/64E)</td><td>MoE Decoder-only</td><td>1.2T</td><td>96.6B</td></tr></tbody></table>
2112.06905,2112.06905_55,GLaM Efficient Scaling of Language Models with Mixture-of-Experts.pdf,"<table><caption>Table 11. Scores of GLaM (64B/64E), GPT-3 and Gopher across all 29 benchmarks. We include the significantly larger and more computationally expensive Gopher and Megatron-NLG models for reference.</caption><thead><tr><th>Dataset</th><th>Tokens (B)</th><th>Weight in mixture</th></tr></thead><tbody><tr><td>Filtered Webpages</td><td>143</td><td>0.42</td></tr><tr><td>Wikipedia</td><td>3</td><td>0.06</td></tr><tr><td>Conversations</td><td>174</td><td>0.28</td></tr><tr><td>Forums</td><td>247</td><td>0.02</td></tr><tr><td>Books</td><td>390</td><td>0.20</td></tr><tr><td>News</td><td>650</td><td>0.02</td></tr></tbody></table>"
2112.06905,2112.06905_56,GLaM Efficient Scaling of Language Models with Mixture-of-Experts.pdf,"<table><caption>Table 12. Zero-shot scores on all 29 benchmarks for GPT3 and different GLaM MoE and dense models.</caption><thead><tr><th>GLaM Model</th><th>= Type</th><th>—Mparams_—</th><th>Mact-params</th><th>=D</th><th>M</th><th>A</th><th>Meads</th><th>head</th><th>EF</th></tr></thead><tbody><tr><td>0.1B 0.1B/64E</td><td>Dense MoE</td><td>130M 1.9B</td><td>130M 145M</td><td>2</td><td>768</td><td>3,072</td><td></td><td>64</td><td>- 64</td></tr><tr><td>1.7B</td><td>Dense</td><td>1.7B</td><td>1.700B</td><td></td><td></td><td></td><td></td><td></td><td>-</td></tr><tr><td>1.7B/32E</td><td>MoE</td><td>20B</td><td>1.878B</td><td></td><td></td><td></td><td></td><td></td><td>32</td></tr><tr><td>1.7B/64E</td><td>MoE</td><td>27B</td><td>1.879B</td><td>24</td><td>2,048</td><td>8,192</td><td>16</td><td>128</td><td>64</td></tr><tr><td>1.7B/128E</td><td>MoE</td><td>53B</td><td>1.881B</td><td></td><td></td><td></td><td></td><td></td><td>128</td></tr><tr><td>1.7B/256E</td><td>MoE</td><td>105B</td><td>1.886B</td><td></td><td></td><td></td><td></td><td></td><td>256</td></tr><tr><td>8B 8B/64E</td><td>Dense MoE</td><td>8.7B 143B</td><td>8.7B 9.8B</td><td>32</td><td>4,096</td><td>16,384</td><td>32</td><td>128</td><td>- 64</td></tr><tr><td>137B</td><td>Dense</td><td>137B</td><td>137B</td><td>64</td><td>8,192</td><td>65,536</td><td>128</td><td>128</td><td>-</td></tr><tr><td>64B/64E</td><td>MoE</td><td>1.2T</td><td>96.6B</td><td>64</td><td>8,192</td><td>32,768</td><td>128</td><td>128</td><td>64</td></tr></tbody></table>"
2112.06905,2112.06905_57,GLaM Efficient Scaling of Language Models with Mixture-of-Experts.pdf,"<table><caption>Table 13. One-shot scores on all 29 benchmarks for GPT3 and different GLaM MoE and dense models.</caption><thead><tr><th>Model</th><th>TriviaQA (Open-Domain)</th></tr></thead><tbody><tr><td>KG-FiD (large) (Yu et al., 2022) (finetuned, test)</td><td>69.8</td></tr><tr><td>Switch-C (finetuned, dev)</td><td>47.5</td></tr><tr><td>GPT-3 One-shot (dev)</td><td>68.0</td></tr><tr><td>GPT-3 64-shot (test)</td><td>71.2</td></tr><tr><td>GLaM One-shot (test)</td><td>75.0</td></tr><tr><td>GLaM One-shot (dev)</td><td>75.8</td></tr></tbody></table>"
2112.06905,2112.06905_58,GLaM Efficient Scaling of Language Models with Mixture-of-Experts.pdf,<table><caption>Table 14. Few-shot scores on all 29 benchmarks for GPT3 and different GLaM MoE and dense models. We tune the number of shots up to the respective value in each task used by GPT3.</caption><thead><tr><th>Dataset</th><th>Split</th><th>Dirty count</th><th>Total count</th><th>% clean</th></tr></thead><tbody><tr><td>ANLIRI</td><td>validation</td><td>962</td><td>1000</td><td>3.8</td></tr><tr><td>ANLI R2</td><td>validation</td><td>968</td><td>1000</td><td>3.2</td></tr><tr><td>ANLI R3</td><td>validation</td><td>596</td><td>1200</td><td>50.33</td></tr><tr><td>ARC Challenge</td><td>validation</td><td>95</td><td>299</td><td>68.23</td></tr><tr><td>ARC Easy</td><td>validation</td><td>185</td><td>570</td><td>67.54</td></tr><tr><td>BoolQ</td><td>validation</td><td>3013</td><td>3270</td><td>7.86</td></tr><tr><td>CB</td><td>validation</td><td>15</td><td>56</td><td>73.21</td></tr><tr><td>COPA</td><td>validation</td><td>3</td><td>100</td><td>97.0</td></tr><tr><td>CoQa</td><td>test</td><td>375</td><td>500</td><td>25.0</td></tr><tr><td>DROP</td><td>dev</td><td>9361</td><td>9536</td><td>1.84</td></tr><tr><td>HellaSwag</td><td>validation</td><td>1989</td><td>10042</td><td>80.19</td></tr><tr><td>LAMBADA</td><td>test</td><td>1125</td><td>5153</td><td>78.17</td></tr><tr><td>MultiRC</td><td>validation</td><td>3334</td><td>4848</td><td>31.23</td></tr><tr><td>NQs</td><td>validation</td><td>141</td><td>3610</td><td>96.09</td></tr><tr><td>OpenBookQA</td><td>validation</td><td>100</td><td>500</td><td>80.0</td></tr><tr><td>PIQA</td><td>validation</td><td>902</td><td>1838</td><td>50.92</td></tr><tr><td>Quac</td><td>validation</td><td>7353</td><td>7354</td><td>0.01</td></tr><tr><td>RACE-h</td><td>dev</td><td>2552</td><td>3451</td><td>26.05</td></tr><tr><td>RACE-m</td><td>dev</td><td>838</td><td>1436</td><td>41.64</td></tr><tr><td>RTE</td><td>validation</td><td>152</td><td>277</td><td>45.13</td></tr><tr><td>ReCoRD</td><td>validation</td><td>9861</td><td>10000</td><td>1.39</td></tr><tr><td>SQuADv2</td><td>validation</td><td>11234</td><td>11873</td><td>5.38</td></tr><tr><td>StoryCloze</td><td>validation</td><td>1871</td><td>1871</td><td>0.0</td></tr><tr><td>TriviaQA</td><td>validation</td><td>2121</td><td>11313</td><td>81.25</td></tr><tr><td>WSC</td><td>test</td><td>157</td><td>273</td><td>42.49</td></tr><tr><td>WiC</td><td>validation</td><td>46</td><td>638</td><td>92.79</td></tr><tr><td>Winograd</td><td>validation</td><td>70</td><td>104</td><td>32.69</td></tr><tr><td>Winogrande</td><td>test</td><td>6</td><td>1767</td><td>99.66</td></tr></tbody></table>
2112.06905,2112.06905_59,GLaM Efficient Scaling of Language Models with Mixture-of-Experts.pdf,"<table><thead><tr><th>Category</th><th>Prompts</th></tr></thead><tbody><tr><td colspan=""2"">gender {term} was very... {term} would be described as...</td></tr><tr><td>race</td><td>The {term} man was very...” The {term} woman was very...” People would describe the {term} person as...</td></tr></tbody></table>"
2112.06905,2112.06905_60,GLaM Efficient Scaling of Language Models with Mixture-of-Experts.pdf,"<table><thead><tr><th></th><th>“He”</th><th>“She”</th></tr></thead><tbody><tr><td>The top 10</td><td>much (188)</td><td>pretty (232)</td></tr><tr><td>most common _</td><td>great (130)</td><td>little (185)</td></tr><tr><td>descriptive</td><td>well (129)</td><td>much (154)</td></tr><tr><td>words (and</td><td>little (129)</td><td>beautiful (148)</td></tr><tr><td rowspan=""6"">counts).</td><td>good (124)</td><td>always (142)</td></tr><tr><td>always (114)</td><td>good (136)</td></tr><tr><td>black (103)</td><td>black (117)</td></tr><tr><td>even (92)</td><td>never (116)</td></tr><tr><td>many (87)</td><td>even (111)</td></tr><tr><td>also (83)</td><td>well (110)</td></tr></tbody></table>"
2112.06905,2112.06905_61,GLaM Efficient Scaling of Language Models with Mixture-of-Experts.pdf,"<table><thead><tr><th>Term</th><th>Most common descriptive words</th></tr></thead><tbody><tr><td>Asian</td><td>Asian, black, white, polite, even, really, Chinese, good, also, nice</td></tr><tr><td>Black</td><td>white, black, much, even, well, angry, good, also, proud, happy</td></tr><tr><td>White</td><td>white, black, many, even, Indian, much,</td></tr><tr><td></td><td>good, happy, angry, never</td></tr><tr><td>Latinx</td><td>white, black, even, really, also, Spanish, much, well, different, never</td></tr><tr><td>Indian</td><td>Indian, white, black, much, even, differ- ent, happy, really, never, good</td></tr><tr><td>Middle-Eastern</td><td>white, black, even, eastern, polite, really, middle, nice, brown, also</td></tr></tbody></table>"
2112.06905,2112.06905_62,GLaM Efficient Scaling of Language Models with Mixture-of-Experts.pdf,"<table><thead><tr><th>Term</th><th>Most common descriptive words</th></tr></thead><tbody><tr><td>Atheism</td><td>religious, also, bad, likely, really, much,</td></tr><tr><td>Buddhism</td><td>many, moral, even, sure also, generally, many, religious, always,</td></tr><tr><td>Christianity</td><td>often, even, good, first, different religious, also, Christian, many, even, often, different, bad</td></tr><tr><td>Islam</td><td>always, likely, also, religious, even, many, likely, still, different, generally, much, violent</td></tr><tr><td>Hinduism</td><td>generally, also, religious, many, differ- ent, even, often, well, Indian, likely</td></tr><tr><td>Judaism</td><td>Jewish, also, religious, responsible, many, even, well, ferent generally, often, dif-</td></tr></tbody></table>"
2112.06905,2112.06905_63,GLaM Efficient Scaling of Language Models with Mixture-of-Experts.pdf,"<table><thead><tr><th>Ji ame</th><th>Metric eure</th><th>Split P""""</th><th>GPT-3 (175B)</th><th>GLaM (64B/64E)</th><th>GPT-3 (175B)</th><th>GLaM (64B/64E)</th><th>GPT-3 (175B)</th><th>Gopher — (280B)</th><th>Megatron-NLG (530B)</th><th>GLaM (64B/64E)</th></tr></thead><tbody><tr><td>rriviaQA</td><td>acc(em)</td><td>dev</td><td>64.3</td><td>71.3</td><td>68.0</td><td>75.8</td><td>71.2 (64)</td><td>57.1 (64)</td><td>-</td><td>75.8 (1)</td></tr><tr><td>NQs</td><td>acc(em)</td><td>test</td><td>14.6</td><td>24.7</td><td>23.0</td><td>26.3</td><td>29.9 (64)</td><td>28.2 (64)</td><td>-</td><td>32.5 (64)</td></tr><tr><td>NebQS</td><td>acc(em)</td><td>test</td><td>14.4</td><td>19.0</td><td>25.3</td><td>24.4</td><td>41.5 (64)</td><td>-</td><td>-</td><td>41.1 (64)</td></tr><tr><td>-ambada</td><td>acc(em)</td><td>test</td><td>76.2</td><td>64.2</td><td>72.5</td><td>80.9</td><td>86.4(15)</td><td>74.50)</td><td>87.2</td><td>86.6 (9)</td></tr><tr><td>fellaSwag</td><td>ace</td><td>dev</td><td>78.9</td><td>76.6</td><td>78.1</td><td>76.8</td><td>79.3 (20)</td><td>79.2(0)</td><td>82.4</td><td>77.2 (8)</td></tr><tr><td>storyCloze</td><td>acc</td><td>test</td><td>83.2</td><td>82.5</td><td>84.7</td><td>84.0</td><td>87.7 (70)</td><td>-</td><td>=</td><td>86.7 (16)</td></tr><tr><td>Ninograd</td><td>acc</td><td>test</td><td>88.3</td><td>87.2</td><td>89.7</td><td>83.9</td><td>88.6 (7)</td><td>-</td><td>=</td><td>88.6 (2)</td></tr><tr><td>NinoGrande</td><td>ace</td><td>lev</td><td>70.2</td><td>73.5</td><td>73.2</td><td>Bl</td><td>77.1 (16)</td><td>— 70.1(0)</td><td>78.9</td><td>79.2 (16)</td></tr><tr><td>ROP</td><td>fl</td><td>lev</td><td>23.6</td><td>57.3</td><td>34.3</td><td>57.8</td><td>36.5 (20)</td><td>-</td><td>-</td><td>58.6 (2)</td></tr><tr><td>“oQA</td><td>fl</td><td>lev</td><td>81.5</td><td>78.8</td><td>84.0</td><td>79.6</td><td>85.0 (5)</td><td>-</td><td>-</td><td>79.6 (1)</td></tr><tr><td>UAC</td><td>fl</td><td>lev</td><td>41.5</td><td>40.3</td><td>43.4</td><td>42.8</td><td>44,3 (5)</td><td>-</td><td>-</td><td>42.7 (1)</td></tr><tr><td>QuADv2</td><td>fl</td><td>lev</td><td>62.1</td><td>71.1</td><td>64.6</td><td>71.8</td><td>69.8 (16)</td><td>-</td><td>-</td><td>71.8 (10)</td></tr><tr><td>SQuADv2 =</td><td>acc(em)</td><td>dev</td><td>52.6</td><td>64.7</td><td>60.1</td><td>66.5</td><td>64.9 (16)</td><td>-</td><td>-</td><td>67.0 (10)</td></tr><tr><td>2ACE-m</td><td>ace</td><td>test</td><td>58.4</td><td>64.0</td><td>574</td><td>65.5</td><td>58.1 (10)</td><td>75.1 (5)</td><td>-</td><td>66.9 (8)</td></tr><tr><td>2ACE-h</td><td>ace</td><td>test</td><td>45.5</td><td>46.9</td><td>45.9</td><td>48.7</td><td>46.8 (10)</td><td>71.6 (5)</td><td>47.9</td><td>49.3 (2)</td></tr><tr><td>IQA</td><td>ace</td><td>lev</td><td>81.0</td><td>80.4</td><td>80.5</td><td>81.4</td><td>82.3 (50)</td><td>81.8 (0)</td><td>83.2</td><td>81.8 (32)</td></tr><tr><td>\RC-e</td><td>ace</td><td>test</td><td>68.8</td><td>71.6</td><td>712</td><td>76.6</td><td>70.1 (50)</td><td>-</td><td>-</td><td>78.9 (16)</td></tr><tr><td>\RC-c</td><td>ace</td><td>test</td><td>51.4</td><td>48.0</td><td>53.2</td><td>50.3</td><td>51.5 (50)</td><td>-</td><td>-</td><td>52.0 (3)</td></tr><tr><td>YpenbookQA</td><td>ace</td><td>test</td><td>57.6</td><td>53.4</td><td>58.8</td><td>55.2</td><td>65.4 (100)</td><td>-</td><td>-</td><td>63.0 (32)</td></tr><tr><td>3001Q</td><td>ace</td><td>dev</td><td>60.5</td><td>83.1</td><td>16.7</td><td>82.8</td><td>77.5 (32)</td><td>-</td><td>84.8</td><td>83.1 (8)</td></tr><tr><td>Yopa</td><td>ace</td><td>dev</td><td>91.0</td><td>90.0</td><td>87.0</td><td>92.0</td><td>92.0 (32)</td><td>-</td><td>-</td><td>93.0 (16)</td></tr><tr><td>ATE</td><td>ace</td><td>dev</td><td>63.5</td><td>67.9</td><td>70.4</td><td>71.5</td><td>72.9 (32)</td><td>-</td><td>-</td><td>76.2 (8)</td></tr><tr><td>NiC</td><td>ace</td><td>dev</td><td>0.0</td><td>50.3</td><td>48.6</td><td>52.7</td><td>55.3 (32)</td><td>-</td><td>58.5</td><td>56.3 (4)</td></tr><tr><td>Multire</td><td>fla</td><td>dev</td><td>72.9</td><td>73.7</td><td>72.9</td><td>74.7</td><td>74.8 (32)</td><td>-</td><td>-</td><td>77.5 (4)</td></tr><tr><td>NSC</td><td>ace</td><td>dev</td><td>65.4</td><td>85.3</td><td>69.2</td><td>83.9</td><td>75.0 (32)</td><td>-</td><td>-</td><td>85.6 (2)</td></tr><tr><td>2eCoRD</td><td>ace</td><td>dev</td><td>90.2</td><td>90.3</td><td>90.2</td><td>90.3</td><td>89.0 (32)</td><td>-</td><td>-</td><td>90.6 (2)</td></tr><tr><td>‘B</td><td>ace</td><td>dev</td><td>46.4</td><td>48.2</td><td>64.3</td><td>73.2</td><td>82.1 (32)</td><td>-</td><td>-</td><td>84.0 (8)</td></tr><tr><td>\NLI RI</td><td>ace</td><td>test</td><td>34.6</td><td>39.2</td><td>32.0</td><td>42.4</td><td>36.8 (50)</td><td>-</td><td>-</td><td>44,3 (2)</td></tr><tr><td>\NLI R2</td><td>ace</td><td>test</td><td>35.4</td><td>37.3</td><td>33.9</td><td>40.0</td><td>34.0 (50)</td><td>-</td><td>39.6</td><td>41.2 (10)</td></tr><tr><td>\NLI R3</td><td>ace</td><td>test</td><td>34.5</td><td>41.3</td><td>35.1</td><td>40.8</td><td>40.2 (50)</td><td>-</td><td>-</td><td>44.7 (4)</td></tr><tr><td>\vg NLG</td><td>-</td><td>-</td><td>476</td><td>54.6</td><td>52.9</td><td>58.4</td><td>58.8</td><td>-</td><td>-</td><td>61.6</td></tr></tbody></table>"
2112.06905,2112.06905_64,GLaM Efficient Scaling of Language Models with Mixture-of-Experts.pdf,"<table><thead><tr><th>Name</th><th>Metric</th><th>Split</th><th>0.1B/64E</th><th>1.7B/64E</th><th>8B/64E</th><th>64B/64E</th><th colspan=""4"">0.1B 1.7B 8B 137B</th></tr></thead><tbody><tr><td>TriviaQA</td><td>acc(em)</td><td>dev</td><td>9.42</td><td>44.0</td><td>55.1</td><td>113</td><td>23</td><td>270</td><td>48.1</td><td>64.0</td></tr><tr><td>NQs</td><td>acc (em)</td><td>test</td><td>2.24</td><td>9.2</td><td>11.9</td><td>24.7</td><td>1.1</td><td>5.6</td><td>9.0</td><td>17.3</td></tr><tr><td>WebQS</td><td>acc(em)</td><td>test</td><td>3.44</td><td>8.3</td><td>10.7</td><td>19.0</td><td>0.7</td><td>5.9</td><td>17</td><td>13.8</td></tr><tr><td>Lambada</td><td>acc(em)</td><td>test</td><td>41.4</td><td>63.7</td><td>67.3</td><td>64.2</td><td>37.8</td><td>60.1</td><td>69.3</td><td>70.9</td></tr><tr><td>HellaSwag</td><td>ace</td><td>lev</td><td>43.1</td><td>65.8</td><td>74.0</td><td>76.6</td><td>34.7</td><td>60.6</td><td>72.2</td><td>76.9</td></tr><tr><td>StoryCloze</td><td>acc</td><td>test</td><td>66.4</td><td>76.2</td><td>78.9</td><td>82.5</td><td>63.3</td><td>75.1</td><td>79.5</td><td>81.1</td></tr><tr><td>Winograd</td><td>acc</td><td>test</td><td>66.3</td><td>80.2</td><td>83.9</td><td>87.2</td><td>67</td><td>78.7</td><td>816</td><td>843</td></tr><tr><td>WinoGrande</td><td>acc</td><td>lev</td><td>51.0</td><td>63.9</td><td>67.8</td><td>73.5</td><td>49.7</td><td>62.6</td><td>70.1</td><td>71.5</td></tr><tr><td>DROP</td><td>fl</td><td>lev</td><td>9.43</td><td>13.4</td><td>16.8</td><td>57.3</td><td>5.67</td><td>140</td><td>17.0</td><td>21.8</td></tr><tr><td>CoQA</td><td>fl</td><td>lev</td><td>45.9</td><td>65.3</td><td>65.5</td><td>78.8</td><td>40.7</td><td>665</td><td>68.7</td><td>72.1</td></tr><tr><td>QuAC</td><td>fl</td><td>lev</td><td>25.2</td><td>32.8</td><td>33.8</td><td>40.3</td><td>254</td><td>33.3</td><td>30.7</td><td>38.3</td></tr><tr><td>SQuADv2</td><td>fl</td><td>lev</td><td>22.9</td><td>49.2</td><td>57.1</td><td>711</td><td>16.8</td><td>449</td><td>55.7</td><td>65.5</td></tr><tr><td>SQuADv2</td><td>acc (em)</td><td>lev</td><td>7.06</td><td>29.6</td><td>38</td><td>64.7</td><td>3.4</td><td>24</td><td>35.8</td><td>48.2</td></tr><tr><td>RACE-m</td><td>acc</td><td>test</td><td>43.4</td><td>56.1</td><td>61.9</td><td>64.0</td><td>40.6</td><td>53.6</td><td>63.0</td><td>67.8</td></tr><tr><td>RACE-h</td><td>acc</td><td>test</td><td>30.4</td><td>40.4</td><td>43.4</td><td>46.9</td><td>294</td><td>40.0</td><td>45.0</td><td>47.2</td></tr><tr><td>PIQA</td><td>ace</td><td>lev</td><td>70.0</td><td>76.9</td><td>78.6</td><td>80.4</td><td>64.4</td><td>73.6</td><td>78.2</td><td>78.5</td></tr><tr><td>ARC-e</td><td>ace</td><td>test</td><td>52.0</td><td>66.2</td><td>66.2</td><td>71.6</td><td>445</td><td>62.2</td><td>67.9</td><td>71.7</td></tr><tr><td>ARC-c</td><td>ace</td><td>test</td><td>26.5</td><td>37.6</td><td>42.8</td><td>48.0</td><td>23.2</td><td>35.1</td><td>42.7</td><td>47.2</td></tr><tr><td>Openbookqa</td><td>acc</td><td>test</td><td>40.0</td><td>46.4</td><td>50.0</td><td>53.4</td><td>36.8</td><td>46.7</td><td>49.8</td><td>52.0</td></tr><tr><td>BoolQ</td><td>ace</td><td>lev</td><td>56.6</td><td>62.7</td><td>72.2</td><td>83.1</td><td>56.6</td><td>56.1</td><td>73.6</td><td>78</td></tr><tr><td>Copa</td><td>acc</td><td>lev</td><td>73</td><td>85</td><td>86</td><td>90</td><td>67</td><td>80</td><td>86</td><td>90</td></tr><tr><td>RTE</td><td>acc</td><td>lev</td><td>45.8</td><td>58.8</td><td>60.3</td><td>67.9</td><td>51.3</td><td>49.1</td><td>63.8</td><td>50.5</td></tr><tr><td>WiC</td><td>ace</td><td>lev</td><td>50.0</td><td>49.8</td><td>49.5</td><td>50.3</td><td>50.8</td><td>50.3</td><td>44</td><td>50.6</td></tr><tr><td>Multirc</td><td>fla</td><td>lev</td><td>57.7</td><td>58.0</td><td>52.4</td><td>73.7</td><td>58.6</td><td>53.0</td><td>39.0</td><td>54.8</td></tr><tr><td>WSC</td><td>ace</td><td>lev</td><td>65.6</td><td>79.3</td><td>81.8</td><td>85.3</td><td>66.3</td><td>77.2</td><td>80.7</td><td>82.8</td></tr><tr><td>ReCoRD</td><td>acc</td><td>lev</td><td>715</td><td>87.1</td><td>88.9</td><td>90.3</td><td>71.6</td><td>86.7</td><td>89.2</td><td>90.3</td></tr><tr><td>CB</td><td>acc</td><td>lev</td><td>66.1</td><td>33.9</td><td>40.7</td><td>48.2</td><td>42.9</td><td>375</td><td>33.9</td><td>42.9</td></tr><tr><td>ANLIRI</td><td>acc</td><td>lev</td><td>34.1</td><td>33.9</td><td>33.4</td><td>39.2</td><td>36.1</td><td>33.2</td><td>34.7</td><td>39.4</td></tr><tr><td>ANLI R2</td><td>acc</td><td>lev</td><td>33.8</td><td>32.4</td><td>34.9</td><td>37.3</td><td>36.7</td><td>33.6</td><td>34.8</td><td>35.7</td></tr><tr><td>ANLI R3</td><td>ace</td><td>lev</td><td>32.8</td><td>34.0</td><td>34.6</td><td>41.3</td><td>34.8</td><td>341</td><td>349</td><td>346</td></tr><tr><td>Avg NLG</td><td>-</td><td>-</td><td>18.6</td><td>35.1</td><td>39.6</td><td>54.6</td><td>149</td><td>31.3</td><td>380</td><td>45.8</td></tr></tbody></table>"
2112.06905,2112.06905_65,GLaM Efficient Scaling of Language Models with Mixture-of-Experts.pdf,<table><thead><tr><th>Name</th><th>Metric</th><th>Split</th><th>0.1B/64E</th><th>1.7B/64E</th><th>8B/64E</th><th>64B/64E</th><th>0.1B</th><th>1.7B</th><th>8B</th><th>137B_</th><th>GPT-3 (175</th></tr></thead><tbody><tr><td>TriviaQA</td><td>acc(em)</td><td>dev</td><td>15.2</td><td>54.1</td><td>65.9</td><td>75.8</td><td>8.3</td><td>36.3</td><td>564</td><td>70.0</td><td>68.0</td></tr><tr><td>NQs</td><td>acc (em) _</td><td>test</td><td>2.5</td><td>10.7</td><td>16.0</td><td>26.3</td><td>119</td><td>65</td><td>10.7</td><td>19.1</td><td>23.0</td></tr><tr><td>WebQS</td><td>acc (em)</td><td>test</td><td>5.9</td><td>13.9</td><td>17.0</td><td>24.4</td><td>344</td><td>93</td><td>116</td><td>18.8</td><td>25.3</td></tr><tr><td>Lambada</td><td>acc (em)</td><td>test</td><td>36.9</td><td>57.4</td><td>64.1</td><td>80.9</td><td>21.8</td><td>52.3</td><td>64.7</td><td>68.5</td><td>72.5</td></tr><tr><td>HellaSwag</td><td>ace</td><td>lev</td><td>43.5</td><td>66.4</td><td>74.0</td><td>76.8</td><td>34.7</td><td>60.5</td><td>72.6</td><td>76.8</td><td>78.1</td></tr><tr><td>StoryCloze</td><td>acc</td><td>test</td><td>67.0</td><td>71.9</td><td>80.0</td><td>84.0</td><td>63.7</td><td>764</td><td>82.1</td><td>82.6</td><td>84.7</td></tr><tr><td>Winograd</td><td>ace</td><td>test</td><td>69.2</td><td>80.2</td><td>85.3</td><td>83.9</td><td>65.6</td><td>80.2</td><td>84</td><td>853</td><td>89.7</td></tr><tr><td>WinoGrande</td><td>acc</td><td>lev</td><td>51.7</td><td>63.5</td><td>68.7</td><td>73.0</td><td>49.8</td><td>62.8</td><td>70.0</td><td>73.1</td><td>73.2</td></tr><tr><td>DROP</td><td>fl</td><td>lev</td><td>16.3</td><td>24.8</td><td>28.4</td><td>57.8</td><td>19.3</td><td>249</td><td>41.2</td><td>494</td><td>34.3</td></tr><tr><td>CoQA</td><td>fl</td><td>lev</td><td>48.3</td><td>72.8</td><td>16</td><td>79.6</td><td>33.3.</td><td>72.7</td><td>744</td><td>78.8</td><td>84.0</td></tr><tr><td>QuAC</td><td>fl</td><td>lev</td><td>28.7</td><td>35.2</td><td>43.1</td><td>42.7</td><td>23.7</td><td>35.7</td><td>35.1</td><td>44.6</td><td>43.4</td></tr><tr><td>SQuADv2</td><td>fl</td><td>lev</td><td>35.5</td><td>69.5</td><td>76.3</td><td>711.8</td><td>34.2</td><td>67.1</td><td>69.2</td><td>70.0</td><td>65.4</td></tr><tr><td>SQuADv2</td><td>acc (em)</td><td>lev</td><td>21.8</td><td>53.6</td><td>60.9</td><td>66.5</td><td>29.0</td><td>50.8</td><td>64.2</td><td>63.7</td><td>60.1</td></tr><tr><td>RACE-m</td><td>acc</td><td>test</td><td>42.7</td><td>60.9</td><td>60.6</td><td>65.5</td><td>43.1</td><td>564</td><td>63.1</td><td>69.0</td><td>57.4</td></tr><tr><td>RACE-h</td><td>ace</td><td>test</td><td>29.1</td><td>41.9</td><td>44.6</td><td>48.7</td><td>294</td><td>40.8</td><td>45.3</td><td>47.7</td><td>45.9</td></tr><tr><td>PIQA</td><td>ace</td><td>lev</td><td>69.0</td><td>76.0</td><td>78.1</td><td>81.4</td><td>63.7</td><td>73.1</td><td>76.3</td><td>79.5</td><td>80.5</td></tr><tr><td>ARC-e</td><td>acc</td><td>test</td><td>53.5</td><td>68.1</td><td>BA</td><td>76.6</td><td>45.9</td><td>63.8</td><td>62.6</td><td>77.2</td><td>71.2</td></tr><tr><td>ARC-c</td><td>acc</td><td>test</td><td>27.0</td><td>39.3</td><td>44.8</td><td>50.3</td><td>24.5</td><td>35.2</td><td>41.5</td><td>50.7</td><td>53.2</td></tr><tr><td>Openbookqa</td><td>acc</td><td>test</td><td>39.6</td><td>47.6</td><td>50.6</td><td>55.2</td><td>37.8</td><td>47.2</td><td>53.0</td><td>55.4</td><td>58.8</td></tr><tr><td>BoolQ</td><td>acc</td><td>lev</td><td>53.6</td><td>62.0</td><td>70.8</td><td>82.8</td><td>55.7</td><td>58.1</td><td>76.4</td><td>77.5</td><td>76.7</td></tr><tr><td>Copa</td><td>acc</td><td>lev</td><td>75</td><td>81</td><td>86</td><td>92</td><td>71</td><td>81</td><td>86</td><td>91</td><td>87</td></tr><tr><td>RTE</td><td>acc</td><td>lev</td><td>53.1</td><td>54.5</td><td>57.0</td><td>71.5</td><td>53.4</td><td>55.2</td><td>62.0</td><td>584</td><td>70.4</td></tr><tr><td>WiC</td><td>acc</td><td>lev</td><td>47.3</td><td>47.0</td><td>48.0</td><td>52.7</td><td>47.3</td><td>46.8</td><td>48.0</td><td>48.7</td><td>48.6</td></tr><tr><td>Multirc</td><td>fla</td><td>lev</td><td>58.5</td><td>59.6</td><td>62.0</td><td>74.7</td><td>56.3</td><td>594</td><td>61.9</td><td>64.2</td><td>72.9</td></tr><tr><td>WSC</td><td>acc</td><td>lev</td><td>67.7</td><td>715</td><td>83.8</td><td>83.9</td><td>63.8</td><td>78.5</td><td>83.0</td><td>86.3</td><td>69.2</td></tr><tr><td>ReCoRD</td><td>acc</td><td>lev</td><td>715</td><td>87.3</td><td>89.0</td><td>90.3</td><td>71.6</td><td>86.2</td><td>89.2</td><td>90.2</td><td>90.1</td></tr><tr><td>CB</td><td>acc</td><td>lev</td><td>41.1</td><td>35.7</td><td>44.6</td><td>73.2</td><td>42.9</td><td>41.1</td><td>304</td><td>48.2</td><td>64.3</td></tr><tr><td>ANLIR1</td><td>acc</td><td>lev</td><td>32.1</td><td>31.1</td><td>32.3</td><td>42.4</td><td>32.5</td><td>314</td><td>31.9</td><td>34.8</td><td>32.0</td></tr><tr><td>ANLI R2</td><td>acc</td><td>lev</td><td>31.1</td><td>30.7</td><td>32.5</td><td>40.0</td><td>30.7</td><td>31.2</td><td>30.7</td><td>32.6</td><td>33.9</td></tr><tr><td>ANLI R3</td><td>acc</td><td>lev</td><td>30.5</td><td>31.6</td><td>34.8</td><td>40.8</td><td>30.9</td><td>30.3</td><td>324</td><td>35.0</td><td>35.1</td></tr><tr><td>Avg NLG</td><td>-</td><td>-</td><td>23.5</td><td>43.6</td><td>49.7</td><td>58.4</td><td>194</td><td>39.5</td><td>47.5</td><td>52.8</td><td>52.7</td></tr></tbody></table>
2112.06905,2112.06905_66,GLaM Efficient Scaling of Language Models with Mixture-of-Experts.pdf,<table><thead><tr><th>Name</th><th>Metric</th><th>Split</th><th>0.1B/64E</th><th>1.7B/64E</th><th>8B/64E</th><th>64B/64E</th><th>0.1B</th><th>1.7B</th><th>8B</th><th>137B_</th><th>GPT-3 (17:</th></tr></thead><tbody><tr><td>TriviaQA</td><td>acc(em)</td><td>dev</td><td>21.7</td><td>60.1</td><td>67.7</td><td>75.8</td><td>8.3</td><td>38.8</td><td>564</td><td>70.0</td><td>71.2</td></tr><tr><td>NQs</td><td>acc (em)</td><td>test</td><td>5.3</td><td>17.7</td><td>24.4</td><td>32.5</td><td>1.50</td><td>9.0</td><td>20.</td><td>27.9</td><td>29.9</td></tr><tr><td>WebQS</td><td>acc (em)</td><td>test</td><td>12.1</td><td>24.4</td><td>29.6</td><td>41.1</td><td>6.90</td><td>9.3</td><td>25.5</td><td>32.9</td><td>41.5</td></tr><tr><td>Lambada</td><td>acc (em)</td><td>test</td><td>36.9</td><td>64.3</td><td>79.0</td><td>86.6</td><td>21.8</td><td>63.0</td><td>77.</td><td>84.2</td><td>86.4</td></tr><tr><td>HellaSwag</td><td>ace</td><td>lev</td><td>45.6</td><td>66.2</td><td>74.0</td><td>71.2</td><td>34.7</td><td>60.7</td><td>72.6</td><td>76.8</td><td>79.3</td></tr><tr><td>StoryCloze —</td><td>acc</td><td>test</td><td>69.4</td><td>80.0</td><td>82.8</td><td>86.7</td><td>63.7</td><td>78.7</td><td>83.7</td><td>85.7</td><td>87.7</td></tr><tr><td>Winograd</td><td>ace</td><td>test</td><td>69.2</td><td>82.8</td><td>85.3</td><td>88.6</td><td>65.6</td><td>805</td><td>85.4</td><td>85.3</td><td>88.6</td></tr><tr><td>WinoGrande</td><td>acc</td><td>lev</td><td>52.6</td><td>66.2</td><td>714</td><td>79.2</td><td>498</td><td>642</td><td>72.3</td><td>76.6</td><td>717</td></tr><tr><td>DROP</td><td>fl</td><td>lev</td><td>23.5</td><td>37.0</td><td>40.0</td><td>58.6</td><td>19.3</td><td>414</td><td>49.4</td><td>49.4</td><td>36.5</td></tr><tr><td>CoQA</td><td>fl</td><td>lev</td><td>48.3</td><td>66.0</td><td>72</td><td>719.6</td><td>33.3</td><td>66.0</td><td>74.4</td><td>78.8</td><td>85.0</td></tr><tr><td>QuAC</td><td>fl</td><td>lev</td><td>26.0</td><td>34.2</td><td>43.1</td><td>42.8</td><td>23.7</td><td>34.3</td><td>35.</td><td>37.2</td><td>44.3</td></tr><tr><td>SQuADv2</td><td>fl</td><td>lev</td><td>38.7</td><td>61.8</td><td>67.1</td><td>71.8</td><td>34.2</td><td>60.0</td><td>69.6</td><td>70.0</td><td>69.8</td></tr><tr><td>SQuADv2</td><td>acc (em)</td><td>lev</td><td>32.7</td><td>55.5</td><td>60.9</td><td>67.0</td><td>29.0</td><td>53.9</td><td>64.2</td><td>63.7</td><td>64.9</td></tr><tr><td>RACE-m</td><td>ace</td><td>test</td><td>41.8</td><td>53.6</td><td>60.6</td><td>66.9</td><td>43.1</td><td>565</td><td>56</td><td>65.1</td><td>58.1</td></tr><tr><td>RACE-h</td><td>ace</td><td>test</td><td>31.5</td><td>40.2</td><td>44.6</td><td>49.3</td><td>295</td><td>408</td><td>43</td><td>48.1</td><td>46.8</td></tr><tr><td>PIQA</td><td>ace</td><td>lev</td><td>69.0</td><td>76.1</td><td>78.1</td><td>81.8</td><td>64.2</td><td>73.1</td><td>77</td><td>80.8</td><td>82.3</td></tr><tr><td>ARC-e</td><td>ace</td><td>test</td><td>57.8</td><td>70.1</td><td>75.3</td><td>78.9</td><td>48.9</td><td>66.0</td><td>74</td><td>79.0</td><td>70.1</td></tr><tr><td>ARC-c</td><td>ace</td><td>test</td><td>29.7</td><td>38.3</td><td>45.5</td><td>52.0</td><td>24.8</td><td>35.2</td><td>41.5</td><td>45.7</td><td>51.5</td></tr><tr><td>Openbookga</td><td>acc</td><td>test</td><td>41.6</td><td>49.6</td><td>53.0</td><td>63.0</td><td>37.8</td><td>54</td><td>54.0</td><td>58.8</td><td>65.4</td></tr><tr><td>BoolQ</td><td>ace</td><td>lev</td><td>53.6</td><td>62.0</td><td>70.5</td><td>83.1</td><td>59.9</td><td>63.1</td><td>76.4</td><td>80.5</td><td>715</td></tr><tr><td>Copa</td><td>ace</td><td>lev</td><td>75</td><td>82</td><td>88</td><td>93.0</td><td>71</td><td>83</td><td>92.0</td><td>91.0</td><td>92.0</td></tr><tr><td>RTE</td><td>ace</td><td>lev</td><td>53.1</td><td>54.5</td><td>60.0</td><td>76.2</td><td>54.9</td><td>55.2</td><td>64.0</td><td>63.9</td><td>72.9</td></tr><tr><td>WiC</td><td>ace</td><td>lev</td><td>49.4</td><td>51.3</td><td>53.3</td><td>56.3</td><td>51.9</td><td>50.9</td><td>50.0</td><td>53.6</td><td>55.3</td></tr><tr><td>Multirc</td><td>fla</td><td>lev</td><td>58.5</td><td>59.7</td><td>62.0</td><td>715</td><td>56.3</td><td>594</td><td>61.5</td><td>68.1</td><td>74.8</td></tr><tr><td>WSC</td><td>ace</td><td>lev</td><td>67.7</td><td>80.4</td><td>83.8</td><td>85.6</td><td>65.6</td><td>80.0</td><td>82.0</td><td>87.4</td><td>75.0</td></tr><tr><td>ReCoRD</td><td>ace</td><td>lev</td><td>715</td><td>87.3</td><td>89.0</td><td>90.6</td><td>71.8</td><td>86.2</td><td>89.0</td><td>90.5</td><td>89.0</td></tr><tr><td>CB</td><td>ace</td><td>lev</td><td>43.0</td><td>53.6</td><td>60.7</td><td>84.0</td><td>429</td><td>554</td><td>58</td><td>53.6</td><td>82.1</td></tr><tr><td>ANLIRI</td><td>ace</td><td>lev</td><td>34.3</td><td>31.4</td><td>34.0</td><td>44.3</td><td>33.5</td><td>33.1</td><td>33.2</td><td>35.8</td><td>36.8</td></tr><tr><td>ANLI R2</td><td>ace</td><td>lev</td><td>32.3</td><td>33.0</td><td>32.0</td><td>41.2</td><td>34.4</td><td>33.7</td><td>33.9</td><td>35.6</td><td>34.0</td></tr><tr><td>ANLI R3</td><td>ace</td><td>lev</td><td>33.9</td><td>35.8</td><td>33.0</td><td>44.7</td><td>32.9</td><td>33.3</td><td>35.0</td><td>34.7</td><td>40.2</td></tr><tr><td>Avg NLG</td><td>-</td><td>-</td><td>27.2</td><td>46.8</td><td>53.0</td><td>61.6</td><td>19.8</td><td>42.7</td><td>524</td><td>57.1</td><td>58.8</td></tr></tbody></table>
2112.06905,2112.06905_67,GLaM Efficient Scaling of Language Models with Mixture-of-Experts.pdf,References
